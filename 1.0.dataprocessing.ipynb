{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x0 = [] # 案情描述，事实部分\n",
    "train_y01 = [] # imprisonment 刑期类型及长短\n",
    "train_y02 = [] # 被告人，背叛罪名，金钱惩罚，相关法律条文\n",
    "\n",
    "for line in open('CAIL_data/data_train.json', 'r'):\n",
    "    item = json.loads(line)\n",
    "    train_x0.append(item['fact'])    # 案情描述，事实部分 \n",
    "    imprisonment = item['meta'].pop('term_of_imprisonment') # imprisonment 刑期类型及长短\n",
    "    train_y01.append(imprisonment)\n",
    "    train_y02.append(item['meta']) # 被告人，背叛罪名，金钱惩罚，相关法律条文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>death_penalty</th>\n",
       "      <th>imprisonment</th>\n",
       "      <th>life_imprisonment</th>\n",
       "      <th>accusation</th>\n",
       "      <th>criminals</th>\n",
       "      <th>punish_of_money</th>\n",
       "      <th>relevant_articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>昌宁县人民检察院指控，2014年4月19日下午16时许，被告人段某驾拖车经过鸡飞乡澡塘街子，...</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>[故意伤害]</td>\n",
       "      <td>[段某]</td>\n",
       "      <td>0</td>\n",
       "      <td>[234]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>公诉机关指控,2015年11月10日晚9时许，被告人李某的妹妹李某某与被害人华某某在桦川县悦...</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>[故意伤害]</td>\n",
       "      <td>[李某]</td>\n",
       "      <td>0</td>\n",
       "      <td>[234]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>贵州省平坝县人民检察院指控：2014年4月9日下午，被告人王某丁与其堂哥王4某（另案处理）假...</td>\n",
       "      <td>False</td>\n",
       "      <td>42</td>\n",
       "      <td>False</td>\n",
       "      <td>[故意伤害]</td>\n",
       "      <td>[王某丁]</td>\n",
       "      <td>0</td>\n",
       "      <td>[292, 234]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>经审理查明：2014年5月6日14时许，被告人叶某某驾车途径赤壁市赵李桥镇胜利街涵洞时，被在...</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>[故意伤害]</td>\n",
       "      <td>[叶某某]</td>\n",
       "      <td>0</td>\n",
       "      <td>[234]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>安阳县人民检察院指控：2014年4月27日上午11时许，宋某甲在安阳县吕村镇翟奇务村被告人梁...</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>[故意伤害]</td>\n",
       "      <td>[梁某甲]</td>\n",
       "      <td>0</td>\n",
       "      <td>[234]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  death_penalty  \\\n",
       "0  昌宁县人民检察院指控，2014年4月19日下午16时许，被告人段某驾拖车经过鸡飞乡澡塘街子，...          False   \n",
       "1  公诉机关指控,2015年11月10日晚9时许，被告人李某的妹妹李某某与被害人华某某在桦川县悦...          False   \n",
       "2  贵州省平坝县人民检察院指控：2014年4月9日下午，被告人王某丁与其堂哥王4某（另案处理）假...          False   \n",
       "3  经审理查明：2014年5月6日14时许，被告人叶某某驾车途径赤壁市赵李桥镇胜利街涵洞时，被在...          False   \n",
       "4  安阳县人民检察院指控：2014年4月27日上午11时许，宋某甲在安阳县吕村镇翟奇务村被告人梁...          False   \n",
       "\n",
       "   imprisonment  life_imprisonment accusation criminals  punish_of_money  \\\n",
       "0            12              False     [故意伤害]      [段某]                0   \n",
       "1            10              False     [故意伤害]      [李某]                0   \n",
       "2            42              False     [故意伤害]     [王某丁]                0   \n",
       "3            12              False     [故意伤害]     [叶某某]                0   \n",
       "4            12              False     [故意伤害]     [梁某甲]                0   \n",
       "\n",
       "  relevant_articles  \n",
       "0             [234]  \n",
       "1             [234]  \n",
       "2        [292, 234]  \n",
       "3             [234]  \n",
       "4             [234]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat([pd.DataFrame(train_x0),pd.DataFrame(train_y01), pd.DataFrame(train_y02)], axis=1)\n",
    "train_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([                  0,     'death_penalty',      'imprisonment',\n",
       "       'life_imprisonment',        'accusation',         'criminals',\n",
       "         'punish_of_money', 'relevant_articles'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_facts = train_df[0]\n",
    "train_label = train_df['relevant_articles']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    昌宁县人民检察院指控，2014年4月19日下午16时许，被告人段某驾拖车经过鸡飞乡澡塘街子，...\n",
       "1    公诉机关指控,2015年11月10日晚9时许，被告人李某的妹妹李某某与被害人华某某在桦川县悦...\n",
       "2    贵州省平坝县人民检察院指控：2014年4月9日下午，被告人王某丁与其堂哥王4某（另案处理）假...\n",
       "3    经审理查明：2014年5月6日14时许，被告人叶某某驾车途径赤壁市赵李桥镇胜利街涵洞时，被在...\n",
       "4    安阳县人民检察院指控：2014年4月27日上午11时许，宋某甲在安阳县吕村镇翟奇务村被告人梁...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_facts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import jieba.analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 斯坦福分词，选取有意义的名词，专有名词，和动词\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('/home/jeshe/stanford-corenlp-full-2017-06-09/', lang='zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_document(document):               \n",
    "    result = [i for (i, j) in nlp.pos_tag(document) if j in ['NN', 'NR', 'VV']]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file = open('corpus.txt','w')\n",
    "\n",
    "i = 0\n",
    "corpus = []\n",
    "\n",
    "for fact in train_facts:\n",
    "    i = i+1\n",
    "    line = ' '.join(seg_document(fact))\n",
    "    corpus.append(line)\n",
    "    file.write(line)\n",
    "    print('process %d' %i)\n",
    "\n",
    "file.close()\n",
    "nlp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 已经生成了该有的\n",
    "\n",
    "# file = open('corpus1.txt','w')\n",
    "# file.write(str(corpus))\n",
    "# file.close()\n",
    "\n",
    "# corpus2 = '\\n'.join(corpus)\n",
    "# file = open('corpus2.txt','w')\n",
    "# file.write(str(corpus2))\n",
    "# file.close()\n",
    "\n",
    "# with open('corpus2.txt','r') as f:\n",
    "#     data = f.read()\n",
    "# f.close()\n",
    "# data.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x0 = [] # 案情描述，事实部分\n",
    "valid_y01 = [] # imprisonment 刑期类型及长短\n",
    "valid_y02 = [] # 被告人，背叛罪名，金钱惩罚，相关法律条文\n",
    "\n",
    "for line in open('CAIL_data/data_valid.json', 'r'):\n",
    "    item = json.loads(line)\n",
    "    valid_x0.append(item['fact'])    # 案情描述，事实部分 \n",
    "    imprisonment = item['meta'].pop('term_of_imprisonment') # imprisonment 刑期类型及长短\n",
    "    valid_y01.append(imprisonment)\n",
    "    valid_y02.append(item['meta']) # 被告人，背叛罪名，金钱惩罚，相关法律条文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>death_penalty</th>\n",
       "      <th>imprisonment</th>\n",
       "      <th>life_imprisonment</th>\n",
       "      <th>accusation</th>\n",
       "      <th>criminals</th>\n",
       "      <th>punish_of_money</th>\n",
       "      <th>relevant_articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>公诉机关起诉指控，被告人张某某秘密窃取他人财物，价值2210元，××数额较大，其行为已触犯《...</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>[盗窃]</td>\n",
       "      <td>[张某某]</td>\n",
       "      <td>0</td>\n",
       "      <td>[264]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>孝昌县人民检察院指控：2014年1月4日，被告人邬某在孝昌县城区2路公交车上××被害人晏某白...</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>[盗窃]</td>\n",
       "      <td>[邬某]</td>\n",
       "      <td>1000</td>\n",
       "      <td>[264]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>广东省广州市南沙区人民检察院指控被告人罗某于2015年6月2日到广州市南沙区大岗镇人民路宇航...</td>\n",
       "      <td>False</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>[盗窃]</td>\n",
       "      <td>[罗某]</td>\n",
       "      <td>2000</td>\n",
       "      <td>[264]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>公诉机关指控，2016年3月3日18时许，被告人易某某行至达州市通川区大观园公交车站附近，扒...</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>[盗窃]</td>\n",
       "      <td>[易某某]</td>\n",
       "      <td>1000</td>\n",
       "      <td>[264]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>公诉机关指控：1.2015年8月20日晚上，被告人胡某甲至杭州市淳安县千岛湖镇新安东路112...</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>[盗窃]</td>\n",
       "      <td>[胡某甲]</td>\n",
       "      <td>0</td>\n",
       "      <td>[264]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  death_penalty  \\\n",
       "0  公诉机关起诉指控，被告人张某某秘密窃取他人财物，价值2210元，××数额较大，其行为已触犯《...          False   \n",
       "1  孝昌县人民检察院指控：2014年1月4日，被告人邬某在孝昌县城区2路公交车上××被害人晏某白...          False   \n",
       "2  广东省广州市南沙区人民检察院指控被告人罗某于2015年6月2日到广州市南沙区大岗镇人民路宇航...          False   \n",
       "3  公诉机关指控，2016年3月3日18时许，被告人易某某行至达州市通川区大观园公交车站附近，扒...          False   \n",
       "4  公诉机关指控：1.2015年8月20日晚上，被告人胡某甲至杭州市淳安县千岛湖镇新安东路112...          False   \n",
       "\n",
       "   imprisonment  life_imprisonment accusation criminals  punish_of_money  \\\n",
       "0             2              False       [盗窃]     [张某某]                0   \n",
       "1             4              False       [盗窃]      [邬某]             1000   \n",
       "2             9              False       [盗窃]      [罗某]             2000   \n",
       "3             7              False       [盗窃]     [易某某]             1000   \n",
       "4             7              False       [盗窃]     [胡某甲]                0   \n",
       "\n",
       "  relevant_articles  \n",
       "0             [264]  \n",
       "1             [264]  \n",
       "2             [264]  \n",
       "3             [264]  \n",
       "4             [264]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df = pd.concat([pd.DataFrame(valid_x0),pd.DataFrame(valid_y01), pd.DataFrame(valid_y02)], axis=1)\n",
    "valid_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_facts = valid_df[0]\n",
    "valid_label = valid_df['relevant_articles']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 斯坦福分词，选取有意义的名词，专有名词，和动词\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('/home/jeshe/stanford-corenlp-full-2017-06-09/', lang='zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_document(document):               \n",
    "    result = [i for (i, j) in nlp.pos_tag(document) if j in ['NN', 'NR', 'VV']]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('corpus_valid.txt','w')\n",
    "\n",
    "i = 0\n",
    "corpus_valid = []\n",
    "\n",
    "for fact in valid_facts:\n",
    "    i = i+1\n",
    "    line = ' '.join(seg_document(fact))\n",
    "    corpus_valid.append(line)\n",
    "    file.write(line)\n",
    "    print('process %d' %i)\n",
    "\n",
    "file.close()\n",
    "nlp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 已经生成了该有的valid data\n",
    "\n",
    "file = open('corpus1_valid.txt','w')\n",
    "file.write(str(corpus_valid))\n",
    "file.close()\n",
    "\n",
    "corpus2_valid = '\\n'.join(corpus_valid)\n",
    "file = open('corpus2_valid.txt','w')\n",
    "file.write(str(corpus2_valid))\n",
    "file.close()\n",
    "\n",
    "with open('corpus2_valid.txt','r') as f:\n",
    "    data_valid = f.read()\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17131"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_valid.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x0 = [] # 案情描述，事实部分\n",
    "test_y01 = [] # imprisonment 刑期类型及长短\n",
    "test_y02 = [] # 被告人，背叛罪名，金钱惩罚，相关法律条文\n",
    "\n",
    "for line in open('CAIL_data/data_test.json', 'r'):\n",
    "    item = json.loads(line)\n",
    "    test_x0.append(item['fact'])    # 案情描述，事实部分 \n",
    "    imprisonment = item['meta'].pop('term_of_imprisonment') # imprisonment 刑期类型及长短\n",
    "    test_y01.append(imprisonment)\n",
    "    test_y02.append(item['meta']) # 被告人，背叛罪名，金钱惩罚，相关法律条文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>death_penalty</th>\n",
       "      <th>imprisonment</th>\n",
       "      <th>life_imprisonment</th>\n",
       "      <th>accusation</th>\n",
       "      <th>criminals</th>\n",
       "      <th>punish_of_money</th>\n",
       "      <th>relevant_articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>公诉机关指控：2016年3月28日20时许，被告人颜某在本市洪山区马湖新村足球场马路边捡拾到...</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>[盗窃]</td>\n",
       "      <td>[颜某]</td>\n",
       "      <td>1000</td>\n",
       "      <td>[264]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>天津市静海县人民检察院指控，2014年5月13日上午8时许，被告人李xx在天津市静海县大邱庄...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[盗窃]</td>\n",
       "      <td>[李xx]</td>\n",
       "      <td>0</td>\n",
       "      <td>[264]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>永顺县人民检察院指控，2014年1月11日，被告人李某某与彭某某（另案处理）在永顺县塔卧镇“...</td>\n",
       "      <td>False</td>\n",
       "      <td>144</td>\n",
       "      <td>False</td>\n",
       "      <td>[强奸]</td>\n",
       "      <td>[李某某]</td>\n",
       "      <td>0</td>\n",
       "      <td>[236]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>公诉机关起诉书指控：2016年11月17日凌晨1时许，被告人周某在本县武康街道营盘小区131...</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>[盗窃]</td>\n",
       "      <td>[周某]</td>\n",
       "      <td>5000</td>\n",
       "      <td>[264]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>大名县人民检察院起诉书指控，2014年3月25日9时许，被告人张某在自家庄某处因故与本村席某...</td>\n",
       "      <td>False</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "      <td>[故意伤害]</td>\n",
       "      <td>[张某]</td>\n",
       "      <td>0</td>\n",
       "      <td>[234]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  death_penalty  \\\n",
       "0  公诉机关指控：2016年3月28日20时许，被告人颜某在本市洪山区马湖新村足球场马路边捡拾到...          False   \n",
       "1  天津市静海县人民检察院指控，2014年5月13日上午8时许，被告人李xx在天津市静海县大邱庄...          False   \n",
       "2  永顺县人民检察院指控，2014年1月11日，被告人李某某与彭某某（另案处理）在永顺县塔卧镇“...          False   \n",
       "3  公诉机关起诉书指控：2016年11月17日凌晨1时许，被告人周某在本县武康街道营盘小区131...          False   \n",
       "4  大名县人民检察院起诉书指控，2014年3月25日9时许，被告人张某在自家庄某处因故与本村席某...          False   \n",
       "\n",
       "   imprisonment  life_imprisonment accusation criminals  punish_of_money  \\\n",
       "0             4              False       [盗窃]      [颜某]             1000   \n",
       "1             0              False       [盗窃]     [李xx]                0   \n",
       "2           144              False       [强奸]     [李某某]                0   \n",
       "3             6              False       [盗窃]      [周某]             5000   \n",
       "4            21              False     [故意伤害]      [张某]                0   \n",
       "\n",
       "  relevant_articles  \n",
       "0             [264]  \n",
       "1             [264]  \n",
       "2             [236]  \n",
       "3             [264]  \n",
       "4             [234]  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.concat([pd.DataFrame(test_x0),pd.DataFrame(test_y01), pd.DataFrame(test_y02)], axis=1)\n",
    "test_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32508"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_facts = test_df[0]\n",
    "test_label = test_df['relevant_articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 斯坦福分词，选取有意义的名词，专有名词，和动词\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('/home/jeshe/stanford-corenlp-full-2017-06-09/', lang='zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_document(document):               \n",
    "    result = [i for (i, j) in nlp.pos_tag(document) if j in ['NN', 'NR', 'VV']]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file = open('corpus_test.txt','w')\n",
    "\n",
    "i = 0\n",
    "corpus_test = []\n",
    "\n",
    "for fact in test_facts:\n",
    "    i = i+1\n",
    "    line = ' '.join(seg_document(fact))\n",
    "    corpus_test.append(line)\n",
    "    file.write(line)\n",
    "    print('process %d' %i)\n",
    "\n",
    "file.close()\n",
    "nlp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 已经生成了该有的valid data\n",
    "\n",
    "file = open('corpus1_test.txt','w')\n",
    "file.write(str(corpus_test))\n",
    "file.close()\n",
    "\n",
    "corpus2_test = '\\n'.join(corpus_test)\n",
    "file = open('corpus2_test.txt','w')\n",
    "file.write(str(corpus2_test))\n",
    "file.close()\n",
    "\n",
    "with open('corpus2_test.txt','r') as f:\n",
    "    data_test = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32508"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_test.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import sys\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "from gensim import models\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument#,LabeledSentence\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "\n",
    "#LabeledSentence = gensim.models.doc2vec.LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_facts = train_df[0]\n",
    "train_label = train_df['relevant_articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_label = np.array(train_label)\n",
    "# train_label = list(train_label)\n",
    "# # 检查各个案例中relevant_articles会有几个，最高可能一起案件触犯了12个条文\n",
    "# relevant_articles_no = [len(i) for i in train_df['relevant_articles']]\n",
    "# np.unique(relevant_articles_no)\n",
    "\n",
    "# # 统计最想相关的几个法律条文， 分别是347， 264， 383， 234， 133\n",
    "# rel_art_stas = []\n",
    "# for j in train_df['relevant_articles']: #j遍历154592个item in train_df['relevant_articles']\n",
    "#     for i in j: #每个案件中会有几个罪行\n",
    "#         rel_art_stas.append(i)\n",
    "        \n",
    "# rel_art_stas = pd.Series(rel_art_stas)\n",
    "# #rel_art_stas.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_facts = list(train_facts)\n",
    "train_label = list(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "\n",
    "train_arr_label= MultiLabelBinarizer().fit_transform(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_training = train_facts\n",
    "y_label = train_arr_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_training = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuation = \"\"\".,?!:;(){}[]\"\"\"\n",
    "\n",
    "#treat punctuation as individual words\n",
    "for c in punctuation:\n",
    "    corpus = [z.replace(c, ' %s '%c) for z in corpus]\n",
    "x_training = [z.split() for z in corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154592, (154592, 183))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_training), y_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_int_list(start, stop, length):\n",
    "    start, stop = (int(start), int(stop)) if start <= stop else (int(stop), int(start))\n",
    "    length = int(abs(length)) if length else 0\n",
    "    random_list = []\n",
    "    for i in range(length):\n",
    "        random_list.append(random.randint(start, stop))\n",
    "    return random_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sli = random_int_list(1, 154592, 5000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training_sub = [x_training[i] for i in sli]\n",
    "y_label_sub = [list(y_label[i]) for i in sli]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_label_sub[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_training_sub), len(y_label_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_training_sub, y_label_sub, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 1000, 4000, 1000)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(x_test), len(y_train), len(y_test)#, #sum(y_train), len(y_test), sum(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TaggedReviews(reviews, label_type):\n",
    "        labelized = []\n",
    "        for i,v in enumerate(reviews):\n",
    "            label = '%s_%s'%(label_type,i)\n",
    "            labelized.append(TaggedDocument(v, [label]))\n",
    "        return labelized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2 = TaggedReviews(x_train, 'TRAIN')\n",
    "x_test2 = TaggedReviews(x_test, 'TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Distributed Memory (DM) 和 Distributed bag of words (DBOW) 模型的建立\n",
    "\n",
    "size = 400\n",
    "\n",
    "\n",
    "model_dm = gensim.models.Doc2Vec(min_count=1, window=10, vector_size=size, sample=1e-3, \n",
    "                                 negative=5, workers=3)\n",
    "\n",
    "model_dbow = gensim.models.Doc2Vec(min_count=1, window=10, vector_size=size, sample=1e-3, \n",
    "                                   negative=5, dm=0, workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build model dictionary using all sentence data\n",
    "\n",
    "model_dm.build_vocab(x_train2+x_test2)\n",
    "model_dbow.build_vocab(x_train2+ x_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#进行多次重复训练，每一次都需要对训练数据重新打乱，以提高精度\n",
    "\n",
    "epoch_num = 1\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    print(epoch)\n",
    "    perm = list(np.random.permutation(len(x_train2)))\n",
    "    x_train3 = [x_train2[i] for i in perm]\n",
    "    model_dm.train(x_train3,total_examples = model_dm.corpus_count, \n",
    "                   epochs=model_dm.epochs)\n",
    "    model_dbow.train(x_train3,total_examples = model_dbow.corpus_count, \n",
    "                     epochs=model_dbow.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##读取向量\n",
    "def getVecs(model, corpus, size):\n",
    "    vecs = [np.array(model.docvecs[z.tags[0]]).reshape((1, size)) for z in corpus]\n",
    "    return np.concatenate(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##将训练完成的数据转换为vectors\n",
    "def get_vectors(model_dm,model_dbow,x_train, x_test, size = 400):\n",
    "\n",
    "    #获取训练数据集的文档向量\n",
    "    train_vecs_dm = getVecs(model_dm, x_train, size)\n",
    "    train_vecs_dbow = getVecs(model_dbow, x_train, size)\n",
    "    train_vecs = np.hstack((train_vecs_dm, train_vecs_dbow))\n",
    "    #获取测试数据集的文档向量\n",
    "    test_vecs_dm = getVecs(model_dm, x_test, size)\n",
    "    test_vecs_dbow = getVecs(model_dbow, x_test, size)\n",
    "    test_vecs = np.hstack((test_vecs_dm, test_vecs_dbow))\n",
    "\n",
    "    return train_vecs,test_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vecs,test_vecs = get_vectors(model_dm,model_dbow, x_train2, x_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 800), (1000, 800))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs.shape, test_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('train_vecs.csv', train_vecs, delimiter= ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('y_train.csv', y_train, delimiter= ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('test_vecs.csv', test_vecs, delimiter= ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('y_test.csv', y_test, delimiter= ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_vecs = np.loadtxt('train_vecs.csv', dtype = np.float, delimiter = ',')\n",
    "y_train = np.loadtxt('y_train.csv', dtype = np.float, delimiter = ',')\n",
    "test_vecs = np.loadtxt('test_vecs.csv', dtype = np.float, delimiter = ',')\n",
    "y_test = np.loadtxt('y_test.csv', dtype = np.float, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set parameters:\n",
    "max_features = 1000\n",
    "maxlen = 800\n",
    "batch_size = 32\n",
    "embedding_dims = 500\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 183), (4000, 183), (4000, 800), (1000, 800))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape,y_train.shape, train_vecs.shape,test_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeshe/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "train_vecs = min_max_scaler.fit_transform(train_vecs)*999\n",
    "train_vecs = train_vecs.astype(np.int)\n",
    "test_vecs = min_max_scaler.fit_transform(test_vecs)*999\n",
    "test_vecs = test_vecs.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def recall(y_true, y_pred):\n",
    "    \n",
    "    c1 = K.sum(K.round(K.clip(y_true*y_pred,0,1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0,1)))\n",
    "    return c1/c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(input_dim = max_features,\n",
    "                    output_dim =embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(183))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy',f1])\n",
    "\n",
    "\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#           optimizer= \"adam\",\n",
    "#           metrics=[f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/2\n",
      "4000/4000 [==============================] - 808s 202ms/step - loss: 0.0988 - acc: 0.9760 - f1: 0.0019 - val_loss: 0.0437 - val_acc: 0.9929 - val_f1: 0.0000e+00\n",
      "Epoch 2/2\n",
      "4000/4000 [==============================] - 523s 131ms/step - loss: 0.0394 - acc: 0.9928 - f1: 0.0000e+00 - val_loss: 0.0433 - val_acc: 0.9929 - val_f1: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc1da044da0>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_vecs, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=2, validation_data=(test_vecs, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = model.predict(test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

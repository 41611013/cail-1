{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def init():\n",
    "    f = open('./CAIL_data/law.txt', 'r', encoding = 'utf8')\n",
    "    law = {}\n",
    "    lawname = {}\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        lawname[len(law)] = line.strip()\n",
    "        law[line.strip()] = len(law)\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    f = open('./CAIL_data/accu.txt', 'r', encoding = 'utf8')\n",
    "    accu = {}\n",
    "    accuname = {}\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        accuname[len(accu)] = line.strip()\n",
    "        accu[line.strip()] = len(accu)\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    return law, accu, lawname, accuname\n",
    "\n",
    "\n",
    "law, accu, lawname, accuname = init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClassNum(kind):\n",
    "    global law\n",
    "    global accu\n",
    "\n",
    "    if kind == 'law':\n",
    "        return len(law)\n",
    "    if kind == 'accu':\n",
    "        return len(accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getName(index, kind):\n",
    "    global lawname\n",
    "    global accuname\n",
    "    if kind == 'law':\n",
    "        return lawname[index]\n",
    "\n",
    "    if kind == 'accu':\n",
    "        return accuname[index]\n",
    "\n",
    "\n",
    "def gettime(time):\n",
    "    #将刑期用分类模型来做\n",
    "    v = int(time['imprisonment'])\n",
    "\n",
    "    if time['death_penalty']:\n",
    "        return 0\n",
    "    if time['life_imprisonment']:\n",
    "        return 1\n",
    "    elif v > 10 * 12:\n",
    "        return 2\n",
    "    elif v > 7 * 12:\n",
    "        return 3\n",
    "    elif v > 5 * 12:\n",
    "        return 4\n",
    "    elif v > 3 * 12:\n",
    "        return 5\n",
    "    elif v > 2 * 12:\n",
    "        return 6\n",
    "    elif v > 1 * 12:\n",
    "        return 7\n",
    "    else:\n",
    "        return 8\n",
    "\n",
    "\n",
    "def getlabel(d, kind):\n",
    "    global law\n",
    "    global accu    \n",
    "    # 做单标签\n",
    "    if kind == 'law':\n",
    "    # 返回多个类的第一个\n",
    "        return law[str(d['meta']['relevant_articles'][0])]\n",
    "    if kind == 'accu':\n",
    "        return accu[d['meta']['accusation'][0]]    \n",
    "    if kind == 'time':\n",
    "        return gettime(d['meta']['term_of_imprisonment'])\n",
    "    \n",
    "    return label\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer as TFIDF\n",
    "import json\n",
    "#from predictor import data\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "#import thulac\n",
    "\n",
    "\n",
    "dim = 300\n",
    "\n",
    "# 斯坦福分词，选取有意义的名词，专有名词，和动词\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('/home/jeshe/stanford-corenlp-full-2017-06-09/', lang='zh')\n",
    "\n",
    "def seg_document(document):               \n",
    "    result = [i for (i, j) in nlp.pos_tag(document) if j in ['NN', 'NR', 'VV']]\n",
    "    return result\n",
    "\n",
    "def cut_text(alltext):\n",
    "#     count = 0\n",
    "#     #cut = thulac.thulac(seg_only = True)\n",
    "#     train_text = []\n",
    "#     for text in alltext:\n",
    "#         count += 1\n",
    "#         if count % 2000 == 0:\n",
    "#             print(count)\n",
    "#         #train_text.append(cut.cut(text, text = True))\n",
    "#         train_text.append(seg_document(text))\n",
    "#     nlp.close()\n",
    "    with open('./corpus2.txt','r') as f:\n",
    "        data = f.read()\n",
    "        f.close()\n",
    "        train_text = data.split('\\n')\n",
    "     \n",
    "    return train_text\n",
    "\n",
    "\n",
    "def train_tfidf(train_data):\n",
    "    tfidf = TFIDF(\n",
    "            min_df = 5,\n",
    "            max_features = dim,\n",
    "            ngram_range = (1, 3),\n",
    "            use_idf = 1,\n",
    "            smooth_idf = 1\n",
    "            )\n",
    "    tfidf.fit(train_data)\n",
    "    \n",
    "    return tfidf\n",
    "\n",
    "\n",
    "\n",
    "def read_trainData(path):\n",
    "    fin = open(path, 'r', encoding = 'utf8')\n",
    "    \n",
    "    alltext = []\n",
    "    \n",
    "    accu_label = []\n",
    "    law_label = []\n",
    "    time_label = []\n",
    "\n",
    "    line = fin.readline()\n",
    "    while line:\n",
    "        d = json.loads(line)\n",
    "        alltext.append(d['fact'])\n",
    "        accu_label.append(getlabel(d, 'accu'))\n",
    "        law_label.append(getlabel(d, 'law'))\n",
    "        time_label.append(getlabel(d, 'time'))\n",
    "        line = fin.readline()\n",
    "    fin.close()\n",
    "\n",
    "    return alltext, accu_label, law_label, time_label\n",
    "\n",
    "\n",
    "def train_SVC(vec, label):\n",
    "    SVC = LinearSVC()\n",
    "    SVC.fit(vec, label)\n",
    "    return SVC\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('reading...')\n",
    "    alltext, accu_label, law_label, time_label = read_trainData('./CAIL_data/data_train.json')\n",
    "    print('cut text...')\n",
    "    train_data = cut_text(alltext)\n",
    "    print('train tfidf...')\n",
    "    tfidf = train_tfidf(train_data)\n",
    "    \n",
    "    vec = tfidf.transform(train_data)\n",
    "    \n",
    "    print('accu SVC')\n",
    "    accu = train_SVC(vec, accu_label)\n",
    "    print('law SVC')\n",
    "    law = train_SVC(vec, law_label)\n",
    "    print('time SVC')\n",
    "    time = train_SVC(vec, time_label)\n",
    "\n",
    "    print('saving model')\n",
    "    joblib.dump(tfidf, 'predictor/model/tfidf.model')\n",
    "    joblib.dump(accu, 'predictor/model/accu.model')\n",
    "    joblib.dump(law, 'predictor/model/law.model')\n",
    "    joblib.dump(time, 'predictor/model/time.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "accumodel = joblib.load('predictor/model/accu.model')\n",
    "lawmodel = joblib.load('predictor/model/law.model')\n",
    "timemodel = joblib.load('predictor/model/time.model')\n",
    "tfidf = joblib.load('predictor/model/tfidf.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = tfidf.transform(['公诉 机关 起诉 指控 ， 被告人 张 某某 秘密 窃取 他 人财物 ， 价值 2210 元 ， 盗窃 数额较大 ， 其 行为 已触犯 《 中华人民共和国 刑法 》 × × 之 规定 ， 应当 以 × × 追究其 刑事责任 。 建议 判处 被告人 张 某某 × × 以下 刑罚 ， 并 处罚金 '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumodel.predict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(accu.keys())[list(accu.values()).index(189)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#import thulac\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# 斯坦福分词，选取有意义的名词，专有名词，和动词\n",
    "# from stanfordcorenlp import StanfordCoreNLP\n",
    "# nlp = StanfordCoreNLP('/home/jeshe/stanford-corenlp-full-2017-06-09/', lang='zh')\n",
    "\n",
    "# def seg_document(document):               \n",
    "#     result = [i for (i, j) in nlp.pos_tag(document) if j in ['NN', 'NR', 'VV']]\n",
    "#     return result\n",
    "\n",
    "import jieba\n",
    "class Predictor(object):\n",
    "    def __init__(self):\n",
    "        self.tfidf = joblib.load('predictor/model/tfidf.model')\n",
    "        self.law = joblib.load('predictor/model/law.model')\n",
    "        self.accu = joblib.load('predictor/model/accu.model')\n",
    "        self.time = joblib.load('predictor/model/time.model')\n",
    "        self.batch_size = 1\n",
    "        \n",
    "#         self.accumodel = joblib.load('predictor/model/accu.model')\n",
    "        \n",
    "        #self.cut = thulac.thulac(seg_only = True)\n",
    "        \n",
    "\n",
    "\n",
    "    def predict_law(self, vec):\n",
    "        y = self.law.predict(vec)\n",
    "        return [y[0] + 1]\n",
    "    \n",
    "    def predict_accu(self, vec):\n",
    "        y = self.accu.predict(vec)\n",
    "        return [y[0] + 1]\n",
    "    \n",
    "    def predict_time(self, vec):\n",
    "\n",
    "        y = self.time.predict(vec)[0]\n",
    "        \n",
    "        #返回每一个罪名区间的中位数\n",
    "        if y == 0:\n",
    "            return -2\n",
    "        if y == 1:\n",
    "            return -1\n",
    "        if y == 2:\n",
    "            return 120\n",
    "        if y == 3:\n",
    "            return 102\n",
    "        if y == 4:\n",
    "            return 72\n",
    "        if y == 5:\n",
    "            return 48\n",
    "        if y == 6:\n",
    "            return 30\n",
    "        if y == 7:\n",
    "            return 18\n",
    "        else:\n",
    "            return 6\n",
    "    \n",
    "    \n",
    "    def predict(self, content):\n",
    "        fact = []\n",
    "        #print('text cut...')\n",
    "        #print(content)\n",
    "        for text in content:\n",
    "            fact.append(' '.join(jieba.cut(text)))\n",
    "        #print(fact)\n",
    "#         fact = self.cut.cut(content[0], text = True)\n",
    "#         print(fact[0])\n",
    "        vec = self.tfidf.transform(fact)\n",
    "        ans = {}\n",
    "\n",
    "        ans['accusation'] = self.predict_accu(vec)\n",
    "        ans['articles'] = self.predict_law(vec)\n",
    "        ans['imprisonment'] = self.predict_time(vec)\n",
    "        \n",
    "        #print(ans)\n",
    "        return [ans]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inf = open(os.path.join(data_path, 'data_valid.json'), \"r\", encoding = 'utf8')\n",
    "# ouf = open(os.path.join(output_path, 'out.txt'), \"w\", encoding = 'utf8')\n",
    "# fact = []\n",
    "# for line in inf.readlines(): \n",
    "#     d = json.loads(line)\n",
    "#     fact.append(d['fact'])   \n",
    "#     if len(fact)==1:\n",
    "#         result = solve(fact)\n",
    "#         cnt += len(result)\n",
    "#         fact = []\n",
    "# inf.close()                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.035 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "#from predictor import Predictor\n",
    "\n",
    "data_path = \"./input/\"  # The directory of the input data\n",
    "output_path = \"./output/\"  # The directory of the output data\n",
    "\n",
    "\n",
    "def format_result(result):\n",
    "    rex = {\"accusation\": [], \"articles\": [], \"imprisonment\": -3}\n",
    "\n",
    "    res_acc = []\n",
    "    for x in result[\"accusation\"]:\n",
    "        if not (x is None):\n",
    "            res_acc.append(int(x))\n",
    "    rex[\"accusation\"] = res_acc\n",
    "\n",
    "    if not (result[\"imprisonment\"] is None):\n",
    "        rex[\"imprisonment\"] = int(result[\"imprisonment\"])\n",
    "    else:\n",
    "        rex[\"imprisonment\"] = -3\n",
    "\n",
    "    res_art = []\n",
    "    for x in result[\"articles\"]:\n",
    "        if not (x is None):\n",
    "            res_art.append(int(x))\n",
    "    rex[\"articles\"] = res_art\n",
    "\n",
    "    return rex\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user = Predictor()\n",
    "    cnt = 0\n",
    "\n",
    "\n",
    "    def get_batch():\n",
    "        v = user.batch_size\n",
    "        if not (type(v) is int) or v <= 0:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return v\n",
    "\n",
    "\n",
    "    def solve(fact):\n",
    "        result = user.predict(fact)\n",
    "\n",
    "        for a in range(0, len(result)):\n",
    "            result[a] = format_result(result[a])\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    for file_name in os.listdir(data_path):\n",
    "        inf = open(os.path.join(data_path, file_name), \"r\")\n",
    "        ouf = open(os.path.join(output_path, file_name), \"w\")\n",
    "\n",
    "        fact = []\n",
    "\n",
    "        for line in inf:\n",
    "            fact.append(json.loads(line)[\"fact\"])\n",
    "            if len(fact) == get_batch():\n",
    "                result = solve(fact)\n",
    "                cnt += len(result)\n",
    "                for x in result:\n",
    "                    print(json.dumps(x), file=ouf)\n",
    "                fact = []\n",
    "\n",
    "        if len(fact) != 0:\n",
    "            result = solve(fact)\n",
    "            cnt += len(result)\n",
    "            for x in result:\n",
    "                print(json.dumps(x), file=ouf)\n",
    "            fact = []\n",
    "\n",
    "        ouf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def getlabel(d, kind):\n",
    "    global law\n",
    "    global accu    \n",
    "    # 做单标签\n",
    "    if kind == 'law':\n",
    "    # 返回多个类的第一个\n",
    "        return law[str(d['meta']['relevant_articles'][0])]\n",
    "    if kind == 'accu':\n",
    "        return accu[d['meta']['accusation'][0]]    \n",
    "    if kind == 'time':\n",
    "        return gettime(d['meta']['term_of_imprisonment'])\n",
    "    \n",
    "    return label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_validData(path):\n",
    "    fin = open(path, 'r', encoding = 'utf8')\n",
    "    \n",
    "    alltext = []\n",
    "    \n",
    "    accu_label_truth = []\n",
    "    law_label_truth = []\n",
    "    time_label_truth = []\n",
    "\n",
    "    line = fin.readline()\n",
    "    while line:\n",
    "        d = json.loads(line)\n",
    "        alltext.append(d['fact'])\n",
    "        accu_label_truth.append(getlabel(d, 'accu'))\n",
    "        law_label_truth.append(getlabel(d, 'law'))\n",
    "        time_label_truth.append(getlabel(d, 'time'))\n",
    "        line = fin.readline()\n",
    "    fin.close()\n",
    "\n",
    "    return alltext, accu_label_truth, law_label_truth, time_label_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alltext, accu_label_truth, law_label_truth, time_label_truth = read_validData('./CAIL_data/data_valid.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jud = Judger('./CAIL_data/accu.txt', './CAIL_data/law.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x0 = [] # 案情描述，事实部分\n",
    "valid_y01 = [] # imprisonment 刑期类型及长短\n",
    "valid_y02 = [] # 被告人，背叛罪名，金钱惩罚，相关法律条文\n",
    "\n",
    "for line in open('./CAIL_data/data_valid.json', 'r'):\n",
    "    item = json.loads(line)\n",
    "    #train_x0.append(item['fact'])    # 案情描述，事实部分 \n",
    "    imprisonment = item['meta'].pop('term_of_imprisonment') # imprisonment 刑期类型及长短\n",
    "    valid_y01.append(imprisonment)\n",
    "    valid_y02.append(item['meta']) # 被告人，背叛罪名，金钱惩罚，相关法律条文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.concat([pd.DataFrame(valid_y01), pd.DataFrame(valid_y02)], axis=1)\n",
    "truth = valid_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = set()\n",
    "for name in truth[\"accusation\"]:\n",
    "    s2.add(self.accu_dic[name.replace(\"[\", \"\").replace(\"]\", \"\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "class Judger:\n",
    "    # Initialize Judger, with the path of accusation list and law articles list\n",
    "    def __init__(self, accusation_path, law_path):\n",
    "        self.accu_dic = {}\n",
    "\n",
    "        f = open(accusation_path, \"r\")\n",
    "        self.task1_cnt = 0\n",
    "        for line in f:\n",
    "            self.task1_cnt += 1\n",
    "            self.accu_dic[line[:-1]] = self.task1_cnt\n",
    "\n",
    "        self.law_dic = {}\n",
    "        f = open(law_path, \"r\")\n",
    "        self.task2_cnt = 0\n",
    "        for line in f:\n",
    "            self.task2_cnt += 1\n",
    "            self.law_dic[int(line[:-1])] = self.task2_cnt\n",
    "\n",
    "    # Format the result generated by the Predictor class\n",
    "    @staticmethod\n",
    "    def format_result(result):\n",
    "        rex = {\"accusation\": [], \"articles\": [], \"imprisonment\": -3}\n",
    "\n",
    "        res_acc = []\n",
    "        for x in result[\"accusation\"]:\n",
    "            if not (x is None):\n",
    "                res_acc.append(int(x))\n",
    "        rex[\"accusation\"] = res_acc\n",
    "\n",
    "        if not (result[\"imprisonment\"] is None):\n",
    "            rex[\"imprisonment\"] = int(result[\"imprisonment\"])\n",
    "        else:\n",
    "            rex[\"imprisonment\"] = -3\n",
    "\n",
    "        res_art = []\n",
    "        for x in result[\"articles\"]:\n",
    "            if not (x is None):\n",
    "                res_art.append(int(x))\n",
    "        rex[\"articles\"] = res_art\n",
    "\n",
    "        return rex\n",
    "\n",
    "    # Gen new results according to the truth and users output\n",
    "    def gen_new_result(self, result, truth, label):\n",
    "        s1 = set(label[\"accusation\"])\n",
    "        s2 = set()\n",
    "        for name in truth[\"accusation\"]:\n",
    "            s2.add(self.accu_dic[name.replace(\"[\", \"\").replace(\"]\", \"\")])\n",
    "\n",
    "        for a in range(0, self.task1_cnt):\n",
    "            in1 = (a + 1) in s1\n",
    "            in2 = (a + 1) in s2\n",
    "            if in1:\n",
    "                if in2:\n",
    "                    result[0][a][\"TP\"] += 1\n",
    "                else:\n",
    "                    result[0][a][\"FP\"] += 1\n",
    "            else:\n",
    "                if in2:\n",
    "                    result[0][a][\"FN\"] += 1\n",
    "                else:\n",
    "                    result[0][a][\"TN\"] += 1\n",
    "\n",
    "        s1 = set(label[\"articles\"])\n",
    "        s2 = set()\n",
    "        for name in truth[\"relevant_articles\"]:\n",
    "            s2.add(self.law_dic[name])\n",
    "\n",
    "        for a in range(0, self.task2_cnt):\n",
    "            in1 = (a + 1) in s1\n",
    "            in2 = (a + 1) in s2\n",
    "            if in1:\n",
    "                if in2:\n",
    "                    result[1][a][\"TP\"] += 1\n",
    "                else:\n",
    "                    result[1][a][\"FP\"] += 1\n",
    "            else:\n",
    "                if in2:\n",
    "                    result[1][a][\"FN\"] += 1\n",
    "                else:\n",
    "                    result[1][a][\"TN\"] += 1\n",
    "\n",
    "        result[2][\"cnt\"] += 1\n",
    "        sc = 0\n",
    "        if truth[\"term_of_imprisonment\"][\"death_penalty\"]:\n",
    "            if label[\"imprisonment\"] == -2:\n",
    "                sc = 1\n",
    "        elif truth[\"term_of_imprisonment\"][\"life_imprisonment\"]:\n",
    "            if label[\"imprisonment\"] == -1:\n",
    "                sc = 1\n",
    "        else:\n",
    "            if label[\"imprisonment\"] < 0:\n",
    "                sc = 0\n",
    "            else:\n",
    "                v1 = truth[\"term_of_imprisonment\"][\"imprisonment\"]\n",
    "                v2 = label[\"imprisonment\"]\n",
    "                v = abs(log(v1 + 1) - log(v2 + 1))\n",
    "                if v <= 0.2:\n",
    "                    sc = 1\n",
    "                elif v <= 0.4:\n",
    "                    sc = 0.8\n",
    "                elif v <= 0.6:\n",
    "                    sc = 0.6\n",
    "                elif v <= 0.8:\n",
    "                    sc = 0.4\n",
    "                elif v <= 1.0:\n",
    "                    sc = 0.2\n",
    "                else:\n",
    "                    sc = 0\n",
    "        sc = sc * 1.0\n",
    "        result[2][\"score\"] += sc\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Calculate precision, recall and f1 value\n",
    "    # According to https://github.com/dice-group/gerbil/wiki/Precision,-Recall-and-F1-measure\n",
    "    @staticmethod\n",
    "    def get_value(res):\n",
    "        if res[\"TP\"] == 0:\n",
    "            if res[\"FP\"] == 0 and res[\"FN\"] == 0:\n",
    "                precision = 1.0\n",
    "                recall = 1.0\n",
    "                f1 = 1.0\n",
    "            else:\n",
    "                precision = 0.0\n",
    "                recall = 0.0\n",
    "                f1 = 0.0\n",
    "        else:\n",
    "            precision = 1.0 * res[\"TP\"] / (res[\"TP\"] + res[\"FP\"])\n",
    "            recall = 1.0 * res[\"TP\"] / (res[\"TP\"] + res[\"FN\"])\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "        return precision, recall, f1\n",
    "\n",
    "    # Generate score for the first two subtasks\n",
    "    def gen_score(self, arr):\n",
    "        sumf = 0\n",
    "        y = {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"TN\": 0}\n",
    "        for x in arr:\n",
    "            p, r, f = self.get_value(x)\n",
    "            sumf += f\n",
    "            for z in x.keys():\n",
    "                y[z] += x[z]\n",
    "\n",
    "        _, __, f_ = self.get_value(y)\n",
    "\n",
    "        return (f_ + sumf * 1.0 / len(arr)) / 2.0\n",
    "\n",
    "    # Generatue all scores\n",
    "    def get_score(self, result):\n",
    "        s1 = self.gen_score(result[0])\n",
    "        s2 = self.gen_score(result[1])\n",
    "        s3 = 1.0 * result[2][\"score\"] / result[2][\"cnt\"]\n",
    "        return [s1, s2, s3]\n",
    "\n",
    "    # Test with ground truth path and the user's output path\n",
    "    def test(self, truth_path, output_path):\n",
    "        cnt = 0\n",
    "        result = [[], [], {}]\n",
    "        for a in range(0, self.task1_cnt):\n",
    "            result[0].append({\"TP\": 0, \"FP\": 0, \"TN\": 0, \"FN\": 0})\n",
    "        for a in range(0, self.task2_cnt):\n",
    "            result[1].append({\"TP\": 0, \"FP\": 0, \"TN\": 0, \"FN\": 0})\n",
    "        result[2] = {\"cnt\": 0, \"score\": 0}\n",
    "\n",
    "        for file_name in os.listdir(truth_path):\n",
    "            inf = open(os.path.join(truth_path, file_name), \"r\")\n",
    "            ouf = open(os.path.join(output_path, file_name), \"r\")\n",
    "\n",
    "            for line in inf:\n",
    "                ground_truth = json.loads(line)[\"meta\"]\n",
    "                user_output = json.loads(ouf.readline())\n",
    "\n",
    "                cnt += 1\n",
    "                result = self.gen_new_result(result, ground_truth, user_output)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

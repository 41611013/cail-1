{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import jieba\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    f = open('./CAIL_data/law.txt', 'r', encoding = 'utf8')\n",
    "    law = {}\n",
    "    lawname = {}\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        lawname[len(law)] = line.strip()\n",
    "        law[line.strip()] = len(law)\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    f = open('./CAIL_data/accu.txt', 'r', encoding = 'utf8')\n",
    "    accu = {}\n",
    "    accuname = {}\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        accuname[len(accu)] = line.strip()\n",
    "        accu[line.strip()] = len(accu)\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    return law, accu, lawname, accuname\n",
    "\n",
    "\n",
    "law, accu, lawname, accuname = init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClassNum(kind):\n",
    "    global law\n",
    "    global accu\n",
    "\n",
    "    if kind == 'law':\n",
    "        return len(law)\n",
    "    if kind == 'accu':\n",
    "        return len(accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getName(index, kind):\n",
    "    global lawname\n",
    "    global accuname\n",
    "    if kind == 'law':\n",
    "        return lawname[index]\n",
    "\n",
    "    if kind == 'accu':\n",
    "        return accuname[index]\n",
    "\n",
    "\n",
    "def gettime(time):\n",
    "    #将刑期用分类模型来做\n",
    "    v = int(time['imprisonment'])\n",
    "\n",
    "    if time['death_penalty']:\n",
    "        return 0\n",
    "    if time['life_imprisonment']:\n",
    "        return 1\n",
    "    elif v > 10 * 12:\n",
    "        return 2\n",
    "    elif v > 7 * 12:\n",
    "        return 3\n",
    "    elif v > 5 * 12:\n",
    "        return 4\n",
    "    elif v > 3 * 12:\n",
    "        return 5\n",
    "    elif v > 2 * 12:\n",
    "        return 6\n",
    "    elif v > 1 * 12:\n",
    "        return 7\n",
    "    else:\n",
    "        return 8\n",
    "\n",
    "\n",
    "def getlabel(d, kind):\n",
    "    global law\n",
    "    global accu    \n",
    "    # 做单标签\n",
    "    if kind == 'law':\n",
    "    # 返回多个类的第一个\n",
    "        return law[str(d['meta']['relevant_articles'][0])]\n",
    "    if kind == 'accu':\n",
    "        return accu[d['meta']['accusation'][0]]    \n",
    "    if kind == 'time':\n",
    "        return gettime(d['meta']['term_of_imprisonment'])\n",
    "    \n",
    "    return label\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_trainData(path):\n",
    "    fin = open(path, 'r', encoding = 'utf8')\n",
    "    \n",
    "    alltext = []\n",
    "    \n",
    "    accu_label = []\n",
    "    law_label = []\n",
    "    time_label = []\n",
    "\n",
    "    line = fin.readline()\n",
    "    while line:\n",
    "        d = json.loads(line)\n",
    "        alltext.append(d['fact'])\n",
    "        accu_label.append(getlabel(d, 'accu'))\n",
    "        law_label.append(getlabel(d, 'law'))\n",
    "        time_label.append(getlabel(d, 'time'))\n",
    "        line = fin.readline()\n",
    "    fin.close()\n",
    "\n",
    "    return alltext, accu_label, law_label, time_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alltext, accu_label, law_label, time_label = read_trainData('./CAIL_data/data_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_text(alltext):\n",
    "\n",
    "    train_text = []\n",
    "    for text in alltext:\n",
    "        train_text.append(' '.join(jieba.cut(text)))\n",
    "     \n",
    "    return train_text\n",
    "\n",
    "train_data = cut_text(alltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer as TFIDF\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tfidf(train_data):\n",
    "    tfidf = TFIDF(\n",
    "            min_df = 5,\n",
    "            max_features = 1000,\n",
    "            ngram_range = (1, 3),\n",
    "            use_idf = 1,\n",
    "            smooth_idf = 1\n",
    "            )\n",
    "    tfidf.fit(train_data)\n",
    "    \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = train_tfidf(train_data)\n",
    "vec = tfidf.transform(train_data)\n",
    "\n",
    "joblib.dump(tfidf, 'predictor/model/tfidf.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.shape, len(accu_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as TFIDF\n",
    "import json\n",
    "#from predictor import data\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "#import thulac\n",
    "import jieba\n",
    "# from keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = [1,3,4]\n",
    "\n",
    "# def cate_label(Y):\n",
    "#     # encode class values as integers\n",
    "#     encoder = LabelEncoder()\n",
    "#     encoder.fit(Y)\n",
    "#     encoded_Y = encoder.transform(Y)\n",
    "#     # convert integers to dummy variables (i.e. one hot encoded)\n",
    "#     dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "#     return dummy_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def dnn_model(input_dim, output_dim):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=1000, activation='relu'))\n",
    "    #model.add(Dropout(0.3))\n",
    "    #model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DNN(vecs, label, label_index,batch_size = 2000):            \n",
    "    #  模型学习\n",
    "    # 构建一个CNN模型\n",
    "    label = to_categorical(label, len(label_index))\n",
    "    output_dim =label.shape[1] #output_dim = label.shape[1]\n",
    "    input_dim  = vecs.shape[1]\n",
    "    model = dnn_model(input_dim, output_dim)\n",
    "\n",
    "    print('compile')\n",
    "    model.fit(vecs, label, batch_size=2000, epochs=50, verbose=1, \n",
    "              callbacks=None, validation_split=0.0, validation_data=None, \n",
    "              shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0)\n",
    "    \n",
    "    \n",
    "    print(model.summary())\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile\n",
      "Epoch 1/50\n",
      "154592/154592 [==============================] - 39s 253us/step - loss: 3.5293 - acc: 0.3491\n",
      "Epoch 2/50\n",
      "154592/154592 [==============================] - 38s 249us/step - loss: 1.3712 - acc: 0.6862\n",
      "Epoch 3/50\n",
      "154592/154592 [==============================] - 38s 243us/step - loss: 0.9463 - acc: 0.7665\n",
      "Epoch 4/50\n",
      "154592/154592 [==============================] - 38s 244us/step - loss: 0.7749 - acc: 0.8023\n",
      "Epoch 5/50\n",
      "154592/154592 [==============================] - 38s 245us/step - loss: 0.6706 - acc: 0.8253\n",
      "Epoch 6/50\n",
      "154592/154592 [==============================] - 38s 244us/step - loss: 0.5969 - acc: 0.8432\n",
      "Epoch 7/50\n",
      "154592/154592 [==============================] - 38s 244us/step - loss: 0.5396 - acc: 0.8554\n",
      "Epoch 8/50\n",
      "154592/154592 [==============================] - 38s 249us/step - loss: 0.4909 - acc: 0.8675\n",
      "Epoch 9/50\n",
      "154592/154592 [==============================] - 40s 256us/step - loss: 0.4488 - acc: 0.8779\n",
      "Epoch 10/50\n",
      "154592/154592 [==============================] - 38s 248us/step - loss: 0.4101 - acc: 0.8870\n",
      "Epoch 11/50\n",
      "154592/154592 [==============================] - 38s 248us/step - loss: 0.3778 - acc: 0.8942\n",
      "Epoch 12/50\n",
      "154592/154592 [==============================] - 38s 247us/step - loss: 0.3466 - acc: 0.9032\n",
      "Epoch 13/50\n",
      "154592/154592 [==============================] - 38s 248us/step - loss: 0.3190 - acc: 0.9103\n",
      "Epoch 14/50\n",
      "154592/154592 [==============================] - 38s 248us/step - loss: 0.2917 - acc: 0.9177\n",
      "Epoch 15/50\n",
      "154592/154592 [==============================] - 38s 247us/step - loss: 0.2691 - acc: 0.9241\n",
      "Epoch 16/50\n",
      "154592/154592 [==============================] - 38s 247us/step - loss: 0.2483 - acc: 0.9299\n",
      "Epoch 17/50\n",
      "154592/154592 [==============================] - 40s 256us/step - loss: 0.2275 - acc: 0.9344\n",
      "Epoch 18/50\n",
      "154592/154592 [==============================] - 38s 246us/step - loss: 0.2087 - acc: 0.9399\n",
      "Epoch 19/50\n",
      "154592/154592 [==============================] - 38s 245us/step - loss: 0.1912 - acc: 0.9454\n",
      "Epoch 20/50\n",
      "154592/154592 [==============================] - 38s 246us/step - loss: 0.1757 - acc: 0.9497\n",
      "Epoch 21/50\n",
      "154592/154592 [==============================] - 38s 246us/step - loss: 0.1597 - acc: 0.9542\n",
      "Epoch 22/50\n",
      "154592/154592 [==============================] - 39s 250us/step - loss: 0.1481 - acc: 0.9576\n",
      "Epoch 23/50\n",
      "154592/154592 [==============================] - 39s 254us/step - loss: 0.1349 - acc: 0.9619\n",
      "Epoch 24/50\n",
      "154592/154592 [==============================] - 38s 249us/step - loss: 0.1249 - acc: 0.9648\n",
      "Epoch 25/50\n",
      "154592/154592 [==============================] - 39s 252us/step - loss: 0.1142 - acc: 0.9675\n",
      "Epoch 26/50\n",
      "154592/154592 [==============================] - 38s 249us/step - loss: 0.1061 - acc: 0.9704\n",
      "Epoch 27/50\n",
      "154592/154592 [==============================] - 38s 249us/step - loss: 0.0978 - acc: 0.9730\n",
      "Epoch 28/50\n",
      "154592/154592 [==============================] - 39s 249us/step - loss: 0.0913 - acc: 0.9748\n",
      "Epoch 29/50\n",
      "154592/154592 [==============================] - 39s 250us/step - loss: 0.0846 - acc: 0.9774\n",
      "Epoch 30/50\n",
      "154592/154592 [==============================] - 39s 250us/step - loss: 0.0783 - acc: 0.9792\n",
      "Epoch 31/50\n",
      "154592/154592 [==============================] - 39s 250us/step - loss: 0.0732 - acc: 0.9805\n",
      "Epoch 32/50\n",
      "154592/154592 [==============================] - 39s 250us/step - loss: 0.0680 - acc: 0.9821\n",
      "Epoch 33/50\n",
      "154592/154592 [==============================] - 39s 253us/step - loss: 0.0646 - acc: 0.9836\n",
      "Epoch 34/50\n",
      "154592/154592 [==============================] - 39s 255us/step - loss: 0.0604 - acc: 0.9841\n",
      "Epoch 35/50\n",
      "154592/154592 [==============================] - 38s 248us/step - loss: 0.0572 - acc: 0.9853\n",
      "Epoch 36/50\n",
      "154592/154592 [==============================] - 39s 249us/step - loss: 0.0540 - acc: 0.9862\n",
      "Epoch 37/50\n",
      "154592/154592 [==============================] - 38s 249us/step - loss: 0.0504 - acc: 0.9875\n",
      "Epoch 38/50\n",
      "154592/154592 [==============================] - 38s 248us/step - loss: 0.0479 - acc: 0.9880\n",
      "Epoch 39/50\n",
      "154592/154592 [==============================] - 39s 254us/step - loss: 0.0464 - acc: 0.9880\n",
      "Epoch 40/50\n",
      "154592/154592 [==============================] - 39s 249us/step - loss: 0.0425 - acc: 0.9895\n",
      "Epoch 41/50\n",
      "154592/154592 [==============================] - 38s 248us/step - loss: 0.0417 - acc: 0.9895\n",
      "Epoch 42/50\n",
      "154592/154592 [==============================] - 39s 249us/step - loss: 0.0398 - acc: 0.9898\n",
      "Epoch 43/50\n",
      "154592/154592 [==============================] - 39s 250us/step - loss: 0.0381 - acc: 0.9905\n",
      "Epoch 44/50\n",
      "154592/154592 [==============================] - 39s 249us/step - loss: 0.0366 - acc: 0.9910\n",
      "Epoch 45/50\n",
      "154592/154592 [==============================] - 39s 251us/step - loss: 0.0344 - acc: 0.9914\n",
      "Epoch 46/50\n",
      "154592/154592 [==============================] - 39s 249us/step - loss: 0.0341 - acc: 0.9915\n",
      "Epoch 47/50\n",
      "154592/154592 [==============================] - 38s 249us/step - loss: 0.0325 - acc: 0.9921\n",
      "Epoch 48/50\n",
      "154592/154592 [==============================] - 39s 250us/step - loss: 0.0314 - acc: 0.9922\n",
      "Epoch 49/50\n",
      "154592/154592 [==============================] - 39s 249us/step - loss: 0.0307 - acc: 0.9921\n",
      "Epoch 50/50\n",
      "154592/154592 [==============================] - 39s 249us/step - loss: 0.0297 - acc: 0.9925\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 512)               4096512   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 202)               26058     \n",
      "=================================================================\n",
      "Total params: 4,188,234\n",
      "Trainable params: 4,188,234\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "accu_model = train_DNN(vec,accu_label, accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile\n",
      "Epoch 1/50\n",
      "154592/154592 [==============================] - 38s 249us/step - loss: 3.5014 - acc: 0.3375\n",
      "Epoch 2/50\n",
      "154592/154592 [==============================] - 38s 244us/step - loss: 1.4076 - acc: 0.6749\n",
      "Epoch 3/50\n",
      "154592/154592 [==============================] - 39s 254us/step - loss: 1.0057 - acc: 0.7485\n",
      "Epoch 4/50\n",
      "154592/154592 [==============================] - 38s 245us/step - loss: 0.8437 - acc: 0.7798\n",
      "Epoch 5/50\n",
      "154592/154592 [==============================] - 38s 245us/step - loss: 0.7457 - acc: 0.8005\n",
      "Epoch 6/50\n",
      "154592/154592 [==============================] - 38s 244us/step - loss: 0.6717 - acc: 0.8172\n",
      "Epoch 7/50\n",
      "154592/154592 [==============================] - 38s 245us/step - loss: 0.6143 - acc: 0.8300\n",
      "Epoch 8/50\n",
      "154592/154592 [==============================] - 38s 244us/step - loss: 0.5675 - acc: 0.8406\n",
      "Epoch 9/50\n",
      "154592/154592 [==============================] - 38s 245us/step - loss: 0.5249 - acc: 0.8510\n",
      "Epoch 10/50\n",
      "154592/154592 [==============================] - 38s 245us/step - loss: 0.4849 - acc: 0.8610\n",
      "Epoch 11/50\n",
      "154592/154592 [==============================] - 38s 245us/step - loss: 0.4505 - acc: 0.8692\n",
      "Epoch 12/50\n",
      "154592/154592 [==============================] - 38s 245us/step - loss: 0.4190 - acc: 0.8772\n",
      "Epoch 13/50\n",
      "154592/154592 [==============================] - 38s 245us/step - loss: 0.3880 - acc: 0.8858\n",
      "Epoch 14/50\n",
      "154592/154592 [==============================] - 38s 245us/step - loss: 0.3598 - acc: 0.8933\n",
      "Epoch 15/50\n",
      "154592/154592 [==============================] - 38s 245us/step - loss: 0.3351 - acc: 0.8997\n",
      "Epoch 16/50\n",
      "154592/154592 [==============================] - 38s 245us/step - loss: 0.3096 - acc: 0.9070\n",
      "Epoch 17/50\n",
      "154592/154592 [==============================] - 39s 252us/step - loss: 0.2872 - acc: 0.9137\n",
      "Epoch 18/50\n",
      "154592/154592 [==============================] - 39s 252us/step - loss: 0.2674 - acc: 0.9190\n",
      "Epoch 19/50\n",
      "154592/154592 [==============================] - 40s 260us/step - loss: 0.2458 - acc: 0.9253\n",
      "Epoch 20/50\n",
      "154592/154592 [==============================] - 45s 291us/step - loss: 0.2263 - acc: 0.9309\n",
      "Epoch 21/50\n",
      "154592/154592 [==============================] - 45s 293us/step - loss: 0.2103 - acc: 0.9362\n",
      "Epoch 22/50\n",
      "154592/154592 [==============================] - 45s 290us/step - loss: 0.1927 - acc: 0.9422\n",
      "Epoch 23/50\n",
      "154592/154592 [==============================] - 45s 290us/step - loss: 0.1776 - acc: 0.9470\n",
      "Epoch 24/50\n",
      "154592/154592 [==============================] - 45s 290us/step - loss: 0.1654 - acc: 0.9517\n",
      "Epoch 25/50\n",
      "154592/154592 [==============================] - 46s 297us/step - loss: 0.1520 - acc: 0.9551\n",
      "Epoch 26/50\n",
      "154592/154592 [==============================] - 46s 297us/step - loss: 0.1396 - acc: 0.9589\n",
      "Epoch 27/50\n",
      "154592/154592 [==============================] - 44s 287us/step - loss: 0.1289 - acc: 0.9626\n",
      "Epoch 28/50\n",
      "154592/154592 [==============================] - 45s 289us/step - loss: 0.1197 - acc: 0.9652\n",
      "Epoch 29/50\n",
      "154592/154592 [==============================] - 43s 277us/step - loss: 0.1109 - acc: 0.9684\n",
      "Epoch 30/50\n",
      "154592/154592 [==============================] - 43s 275us/step - loss: 0.1028 - acc: 0.9709\n",
      "Epoch 31/50\n",
      "154592/154592 [==============================] - 43s 277us/step - loss: 0.0961 - acc: 0.9731\n",
      "Epoch 32/50\n",
      "154592/154592 [==============================] - 43s 279us/step - loss: 0.0903 - acc: 0.9755\n",
      "Epoch 33/50\n",
      "154592/154592 [==============================] - 43s 279us/step - loss: 0.0830 - acc: 0.9775\n",
      "Epoch 34/50\n",
      "154592/154592 [==============================] - 43s 279us/step - loss: 0.0788 - acc: 0.9786\n",
      "Epoch 35/50\n",
      "154592/154592 [==============================] - 43s 276us/step - loss: 0.0732 - acc: 0.9805\n",
      "Epoch 36/50\n",
      "154592/154592 [==============================] - 43s 279us/step - loss: 0.0688 - acc: 0.9820\n",
      "Epoch 37/50\n",
      "154592/154592 [==============================] - 43s 277us/step - loss: 0.0649 - acc: 0.9830\n",
      "Epoch 38/50\n",
      "154592/154592 [==============================] - 43s 275us/step - loss: 0.0622 - acc: 0.9837\n",
      "Epoch 39/50\n",
      "154592/154592 [==============================] - 43s 277us/step - loss: 0.0582 - acc: 0.9849\n",
      "Epoch 40/50\n",
      "154592/154592 [==============================] - 43s 275us/step - loss: 0.0556 - acc: 0.9857\n",
      "Epoch 41/50\n",
      "154592/154592 [==============================] - 43s 276us/step - loss: 0.0519 - acc: 0.9867\n",
      "Epoch 42/50\n",
      "154592/154592 [==============================] - 43s 276us/step - loss: 0.0503 - acc: 0.9869\n",
      "Epoch 43/50\n",
      "154592/154592 [==============================] - 43s 279us/step - loss: 0.0483 - acc: 0.9877\n",
      "Epoch 44/50\n",
      "154592/154592 [==============================] - 43s 280us/step - loss: 0.0467 - acc: 0.9881\n",
      "Epoch 45/50\n",
      "154592/154592 [==============================] - 43s 277us/step - loss: 0.0441 - acc: 0.9888\n",
      "Epoch 46/50\n",
      "154592/154592 [==============================] - 43s 278us/step - loss: 0.0432 - acc: 0.9893\n",
      "Epoch 47/50\n",
      "154592/154592 [==============================] - 43s 279us/step - loss: 0.0415 - acc: 0.9895\n",
      "Epoch 48/50\n",
      "154592/154592 [==============================] - 43s 277us/step - loss: 0.0405 - acc: 0.9894\n",
      "Epoch 49/50\n",
      "154592/154592 [==============================] - 43s 278us/step - loss: 0.0374 - acc: 0.9908\n",
      "Epoch 50/50\n",
      "154592/154592 [==============================] - 43s 278us/step - loss: 0.0362 - acc: 0.9912\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 512)               4096512   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 183)               23607     \n",
      "=================================================================\n",
      "Total params: 4,185,783\n",
      "Trainable params: 4,185,783\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "law_model = train_DNN(vec, law_label, law)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile\n",
      "Epoch 1/50\n",
      "154592/154592 [==============================] - 47s 302us/step - loss: 1.4176 - acc: 0.5127\n",
      "Epoch 2/50\n",
      "154592/154592 [==============================] - 37s 239us/step - loss: 1.1600 - acc: 0.5652\n",
      "Epoch 3/50\n",
      "154592/154592 [==============================] - 36s 235us/step - loss: 1.0849 - acc: 0.5909\n",
      "Epoch 4/50\n",
      "154592/154592 [==============================] - 36s 234us/step - loss: 1.0305 - acc: 0.6110\n",
      "Epoch 5/50\n",
      "154592/154592 [==============================] - 36s 234us/step - loss: 0.9734 - acc: 0.6348\n",
      "Epoch 6/50\n",
      "154592/154592 [==============================] - 36s 235us/step - loss: 0.9012 - acc: 0.6645\n",
      "Epoch 7/50\n",
      "154592/154592 [==============================] - 36s 235us/step - loss: 0.8103 - acc: 0.7044\n",
      "Epoch 8/50\n",
      "154592/154592 [==============================] - 36s 235us/step - loss: 0.7028 - acc: 0.7484\n",
      "Epoch 9/50\n",
      "154592/154592 [==============================] - 36s 235us/step - loss: 0.5840 - acc: 0.7964\n",
      "Epoch 10/50\n",
      "154592/154592 [==============================] - 36s 234us/step - loss: 0.4742 - acc: 0.8389\n",
      "Epoch 11/50\n",
      "154592/154592 [==============================] - 36s 235us/step - loss: 0.3739 - acc: 0.8755\n",
      "Epoch 12/50\n",
      "154592/154592 [==============================] - 36s 234us/step - loss: 0.2939 - acc: 0.9043\n",
      "Epoch 13/50\n",
      "154592/154592 [==============================] - 36s 234us/step - loss: 0.2327 - acc: 0.9256\n",
      "Epoch 14/50\n",
      "154592/154592 [==============================] - 36s 234us/step - loss: 0.1868 - acc: 0.9416\n",
      "Epoch 15/50\n",
      "154592/154592 [==============================] - 36s 234us/step - loss: 0.1543 - acc: 0.9522\n",
      "Epoch 16/50\n",
      "154592/154592 [==============================] - 36s 234us/step - loss: 0.1309 - acc: 0.9598\n",
      "Epoch 17/50\n",
      "154592/154592 [==============================] - 36s 234us/step - loss: 0.1131 - acc: 0.9656\n",
      "Epoch 18/50\n",
      "154592/154592 [==============================] - 36s 234us/step - loss: 0.0972 - acc: 0.9708\n",
      "Epoch 19/50\n",
      "154592/154592 [==============================] - 36s 233us/step - loss: 0.0870 - acc: 0.9735\n",
      "Epoch 20/50\n",
      "154592/154592 [==============================] - 36s 234us/step - loss: 0.0784 - acc: 0.9764\n",
      "Epoch 21/50\n",
      "154592/154592 [==============================] - 36s 233us/step - loss: 0.0716 - acc: 0.9783\n",
      "Epoch 22/50\n",
      "154592/154592 [==============================] - 36s 234us/step - loss: 0.0656 - acc: 0.9799\n",
      "Epoch 23/50\n",
      "154592/154592 [==============================] - 36s 233us/step - loss: 0.0618 - acc: 0.9809\n",
      "Epoch 24/50\n",
      "154592/154592 [==============================] - 36s 233us/step - loss: 0.0569 - acc: 0.9825\n",
      "Epoch 25/50\n",
      "154592/154592 [==============================] - 36s 234us/step - loss: 0.0532 - acc: 0.9839\n",
      "Epoch 26/50\n",
      "154592/154592 [==============================] - 36s 233us/step - loss: 0.0510 - acc: 0.9845\n",
      "Epoch 27/50\n",
      "154592/154592 [==============================] - 36s 233us/step - loss: 0.0471 - acc: 0.9856\n",
      "Epoch 28/50\n",
      "154592/154592 [==============================] - 36s 236us/step - loss: 0.0451 - acc: 0.9862\n",
      "Epoch 29/50\n",
      "154592/154592 [==============================] - 42s 271us/step - loss: 0.0438 - acc: 0.9869\n",
      "Epoch 30/50\n",
      "154592/154592 [==============================] - 41s 267us/step - loss: 0.0420 - acc: 0.9871\n",
      "Epoch 31/50\n",
      "154592/154592 [==============================] - 41s 264us/step - loss: 0.0414 - acc: 0.9869\n",
      "Epoch 32/50\n",
      "154592/154592 [==============================] - 41s 264us/step - loss: 0.0393 - acc: 0.9878\n",
      "Epoch 33/50\n",
      "154592/154592 [==============================] - 41s 265us/step - loss: 0.0378 - acc: 0.9882\n",
      "Epoch 34/50\n",
      "154592/154592 [==============================] - 41s 265us/step - loss: 0.0371 - acc: 0.9884\n",
      "Epoch 35/50\n",
      "154592/154592 [==============================] - 41s 265us/step - loss: 0.0357 - acc: 0.9888\n",
      "Epoch 36/50\n",
      "154592/154592 [==============================] - 41s 264us/step - loss: 0.0348 - acc: 0.9891\n",
      "Epoch 37/50\n",
      "154592/154592 [==============================] - 41s 264us/step - loss: 0.0336 - acc: 0.9894\n",
      "Epoch 38/50\n",
      "154592/154592 [==============================] - 41s 264us/step - loss: 0.0337 - acc: 0.9894\n",
      "Epoch 39/50\n",
      "154592/154592 [==============================] - 41s 264us/step - loss: 0.0326 - acc: 0.9896\n",
      "Epoch 40/50\n",
      "154592/154592 [==============================] - 41s 263us/step - loss: 0.0312 - acc: 0.9901\n",
      "Epoch 41/50\n",
      "154592/154592 [==============================] - 41s 263us/step - loss: 0.0312 - acc: 0.9903\n",
      "Epoch 42/50\n",
      "154592/154592 [==============================] - 41s 264us/step - loss: 0.0294 - acc: 0.9909\n",
      "Epoch 43/50\n",
      "154592/154592 [==============================] - 41s 264us/step - loss: 0.0294 - acc: 0.9908\n",
      "Epoch 44/50\n",
      "154592/154592 [==============================] - 41s 264us/step - loss: 0.0284 - acc: 0.9911\n",
      "Epoch 45/50\n",
      "154592/154592 [==============================] - 41s 265us/step - loss: 0.0278 - acc: 0.9911\n",
      "Epoch 46/50\n",
      "154592/154592 [==============================] - 41s 265us/step - loss: 0.0279 - acc: 0.9912\n",
      "Epoch 47/50\n",
      "154592/154592 [==============================] - 41s 265us/step - loss: 0.0273 - acc: 0.9912\n",
      "Epoch 48/50\n",
      "154592/154592 [==============================] - 41s 264us/step - loss: 0.0255 - acc: 0.9919\n",
      "Epoch 49/50\n",
      "154592/154592 [==============================] - 41s 263us/step - loss: 0.0256 - acc: 0.9917\n",
      "Epoch 50/50\n",
      "154592/154592 [==============================] - 41s 265us/step - loss: 0.0259 - acc: 0.9916\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 512)               4096512   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 4,163,337\n",
      "Trainable params: 4,163,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "time_model = train_DNN(vec, time_label, set(time_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved accu model to disk\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "#accu\n",
    "# serialize model to JSON\n",
    "accu_model_json = accu_model.to_json()\n",
    "with open(\"./predictor/model/accu_model.json\", \"w\") as json_file:\n",
    "    json_file.write(accu_model_json)\n",
    "# serialize weights to HDF5\n",
    "accu_model.save_weights(\"./predictor/model/accu_model.h5\")\n",
    "print(\"Saved accu model to disk\")\n",
    " \n",
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('./predictor/model/accu_model.json', 'r')\n",
    "loaded_accu_model_json = json_file.read()\n",
    "json_file.close()\n",
    "accu_model = model_from_json(loaded_accu_model_json)\n",
    "# load weights into new model\n",
    "accu_model.load_weights(\"./predictor/model/accu_model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved law model to disk\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "#law\n",
    "# serialize model to JSON\n",
    "law_model_json = law_model.to_json()\n",
    "with open(\"./predictor/model/law_model.json\", \"w\") as json_file:\n",
    "    json_file.write(law_model_json)\n",
    "# serialize weights to HDF5\n",
    "law_model.save_weights(\"./predictor/model/law_model.h5\")\n",
    "print(\"Saved law model to disk\")\n",
    " \n",
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('./predictor/model/law_model.json', 'r')\n",
    "loaded_law_model_json = json_file.read()\n",
    "json_file.close()\n",
    "law_model = model_from_json(loaded_law_model_json)\n",
    "# load weights into new model\n",
    "law_model.load_weights(\"./predictor/model/law_model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "#time\n",
    "# serialize model to JSON\n",
    "time_model_json = time_model.to_json()\n",
    "with open(\"./predictor/model/time_model.json\", \"w\") as json_file:\n",
    "    json_file.write(time_model_json)\n",
    "# serialize weights to HDF5\n",
    "time_model.save_weights(\"./predictor/model/time_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n",
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('./predictor/model/time_model.json', 'r')\n",
    "loaded_time_model_json = json_file.read()\n",
    "json_file.close()\n",
    "time_model = model_from_json(loaded_time_model_json)\n",
    "# load weights into new model\n",
    "time_model.load_weights(\"./predictor/model/time_model.h5\")\n",
    "print(\"Loaded model from disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "\n",
    "\n",
    "def getCNN(max_sequence_length, word_index, embedding_dim, embedding_matrix, output_dim, filters = [3,4,5]):\n",
    "    \n",
    "    print('call getCNN')\n",
    "    print(len(word_index), output_dim)\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Conv1D(64, 3, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_dim, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_CNN(vecs, label, label_index,batch_size = 2000):            \n",
    "    #  模型学习\n",
    "    # 构建一个CNN模型\n",
    "    label = to_categorical(label, len(label_index))\n",
    "    output_dim =label.shape[1] #output_dim = label.shape[1]\n",
    "    model = getCNN(dataSet.max_sequence_length,\n",
    "                   dataSet.word_index,\n",
    "                   dataSet.embedding_dim,\n",
    "                   dataSet.embedding_matrix, \n",
    "                   output_dim)\n",
    "\n",
    "    print('compile')\n",
    "    \n",
    "    model.fit(vecs, label, batch_size=2000, epochs=10, verbose=1, \n",
    "              callbacks=None, validation_split=0.0, validation_data=None, \n",
    "              shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0)\n",
    "    \n",
    "    \n",
    "    print(model.summary())\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dtt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# 斯坦福分词，选取有意义的名词，专有名词，和动词\n",
    "# from stanfordcorenlp import StanfordCoreNLP\n",
    "# nlp = StanfordCoreNLP('/home/jeshe/stanford-corenlp-full-2017-06-09/', lang='zh')\n",
    "\n",
    "# def seg_document(document):               \n",
    "#     result = [i for (i, j) in nlp.pos_tag(document) if j in ['NN', 'NR', 'VV']]\n",
    "#     return result\n",
    "\n",
    "import jieba\n",
    "class Predictor(object):\n",
    "    def __init__(self):\n",
    "        self.tfidf = joblib.load('predictor/model/tfidf.model')\n",
    "        \n",
    "        # load json and create model\n",
    "        json_file = open('./predictor/model/accu_model.json', 'r')\n",
    "        loaded_accu_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        self.accu_model = model_from_json(loaded_accu_model_json)\n",
    "        # load weights into new model\n",
    "        self.accu_model.load_weights(\"./predictor/model/accu_model.h5\")\n",
    "        print(\"Loaded accu model from disk\")\n",
    "        \n",
    "        json_file = open('./predictor/model/law_model.json', 'r')\n",
    "        loaded_law_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        self.law_model = model_from_json(loaded_law_model_json)\n",
    "        # load weights into new model\n",
    "        self.law_model.load_weights(\"./predictor/model/law_model.h5\")\n",
    "        print(\"Loaded law model from disk\")\n",
    "\n",
    "        json_file = open('./predictor/model/time_model.json', 'r')\n",
    "        loaded_time_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        self.time_model = model_from_json(loaded_time_model_json)\n",
    "        # load weights into new model\n",
    "        self.time_model.load_weights(\"./predictor/model/time_model.h5\")\n",
    "        print(\"Loaded time model from disk\")\n",
    "\n",
    "        \n",
    "        \n",
    "#         self.law = joblib.load('predictor/model/law.model')\n",
    "#         self.accu = joblib.load('predictor/model/accu.model')\n",
    "#         self.time = joblib.load('predictor/model/time.model')\n",
    "        self.batch_size = 1\n",
    "        \n",
    "#         self.accumodel = joblib.load('predictor/model/accu.model')\n",
    "        \n",
    "        #self.cut = thulac.thulac(seg_only = True)\n",
    "        \n",
    "\n",
    "\n",
    "    def predict_law(self, vec):\n",
    "        y = self.law_model.predict(vec)\n",
    "        y = np.argmax(y, axis=1)\n",
    "#         return y\n",
    "        return [y[0] + 1]\n",
    "    \n",
    "    def predict_accu(self, vec):\n",
    "        y = self.accu_model.predict(vec)\n",
    "        y = np.argmax(y, axis=1)\n",
    "#         return y\n",
    "        return [y[0] + 1]\n",
    "    \n",
    "    def predict_time(self, vec):\n",
    "\n",
    "        y = self.time_model.predict(vec)[0]\n",
    "        y = np.argmax(y)\n",
    "        \n",
    "        #返回每一个罪名区间的中位数\n",
    "        if y == 0:\n",
    "            return -2\n",
    "        if y == 1:\n",
    "            return -1\n",
    "        if y == 2:\n",
    "            return 120\n",
    "        if y == 3:\n",
    "            return 102\n",
    "        if y == 4:\n",
    "            return 72\n",
    "        if y == 5:\n",
    "            return 48\n",
    "        if y == 6:\n",
    "            return 30\n",
    "        if y == 7:\n",
    "            return 18\n",
    "        else:\n",
    "            return 6\n",
    "    \n",
    "    \n",
    "    def predict(self, content):\n",
    "        fact = []\n",
    "        #print('text cut...')\n",
    "        #print(content)\n",
    "        for text in content:\n",
    "            fact.append(' '.join(jieba.cut(text)))\n",
    "        #print(fact)\n",
    "#         fact = self.cut.cut(content[0], text = True)\n",
    "#         print(fact[0])\n",
    "        vec = self.tfidf.transform(fact)\n",
    "        ans = {}\n",
    "\n",
    "        ans['accusation'] = self.predict_accu(vec)\n",
    "        ans['articles'] = self.predict_law(vec)\n",
    "        ans['imprisonment'] = self.predict_time(vec)\n",
    "        \n",
    "        #print(ans)\n",
    "        return [ans]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded accu model from disk\n",
      "Loaded law model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded time model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.288 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "#from predictor import Predictor\n",
    "\n",
    "data_path = \"./input/\"  # The directory of the input data\n",
    "output_path = \"./output/\"  # The directory of the output data\n",
    "\n",
    "\n",
    "def format_result(result):\n",
    "    rex = {\"accusation\": [], \"articles\": [], \"imprisonment\": -3}\n",
    "\n",
    "    res_acc = []\n",
    "    for x in result[\"accusation\"]:\n",
    "        if not (x is None):\n",
    "            res_acc.append(int(x))\n",
    "    rex[\"accusation\"] = res_acc\n",
    "\n",
    "    if not (result[\"imprisonment\"] is None):\n",
    "        rex[\"imprisonment\"] = int(result[\"imprisonment\"])\n",
    "    else:\n",
    "        rex[\"imprisonment\"] = -3\n",
    "\n",
    "    res_art = []\n",
    "    for x in result[\"articles\"]:\n",
    "        if not (x is None):\n",
    "            res_art.append(int(x))\n",
    "    rex[\"articles\"] = res_art\n",
    "\n",
    "    return rex\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user = Predictor()\n",
    "    cnt = 0\n",
    "\n",
    "\n",
    "    def get_batch():\n",
    "        v = user.batch_size\n",
    "        if not (type(v) is int) or v <= 0:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return v\n",
    "\n",
    "\n",
    "    def solve(fact):\n",
    "        result = user.predict(fact)\n",
    "\n",
    "        for a in range(0, len(result)):\n",
    "            result[a] = format_result(result[a])\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    for file_name in os.listdir(data_path):\n",
    "        inf = open(os.path.join(data_path, file_name), \"r\")\n",
    "        ouf = open(os.path.join(output_path, file_name), \"w\")\n",
    "\n",
    "        fact = []\n",
    "\n",
    "        for line in inf:\n",
    "            fact.append(json.loads(line)[\"fact\"])\n",
    "            if len(fact) == get_batch():\n",
    "                result = solve(fact)\n",
    "                cnt += len(result)\n",
    "                for x in result:\n",
    "                    print(json.dumps(x), file=ouf)\n",
    "                fact = []\n",
    "\n",
    "        if len(fact) != 0:\n",
    "            result = solve(fact)\n",
    "            cnt += len(result)\n",
    "            for x in result:\n",
    "                print(json.dumps(x), file=ouf)\n",
    "            fact = []\n",
    "\n",
    "        ouf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "class Judger:\n",
    "    # Initialize Judger, with the path of accusation list and law articles list\n",
    "    def __init__(self, accusation_path, law_path):\n",
    "        self.accu_dic = {}\n",
    "\n",
    "        f = open(accusation_path, \"r\")\n",
    "        self.task1_cnt = 0\n",
    "        for line in f:\n",
    "            self.task1_cnt += 1\n",
    "            self.accu_dic[line[:-1]] = self.task1_cnt\n",
    "\n",
    "        self.law_dic = {}\n",
    "        f = open(law_path, \"r\")\n",
    "        self.task2_cnt = 0\n",
    "        for line in f:\n",
    "            self.task2_cnt += 1\n",
    "            self.law_dic[int(line[:-1])] = self.task2_cnt\n",
    "\n",
    "    # Format the result generated by the Predictor class\n",
    "    @staticmethod\n",
    "    def format_result(result):\n",
    "        rex = {\"accusation\": [], \"articles\": [], \"imprisonment\": -3}\n",
    "\n",
    "        res_acc = []\n",
    "        for x in result[\"accusation\"]:\n",
    "            if not (x is None):\n",
    "                res_acc.append(int(x))\n",
    "        rex[\"accusation\"] = res_acc\n",
    "\n",
    "        if not (result[\"imprisonment\"] is None):\n",
    "            rex[\"imprisonment\"] = int(result[\"imprisonment\"])\n",
    "        else:\n",
    "            rex[\"imprisonment\"] = -3\n",
    "\n",
    "        res_art = []\n",
    "        for x in result[\"articles\"]:\n",
    "            if not (x is None):\n",
    "                res_art.append(int(x))\n",
    "        rex[\"articles\"] = res_art\n",
    "\n",
    "        return rex\n",
    "\n",
    "    # Gen new results according to the truth and users output\n",
    "    def gen_new_result(self, result, truth, label):\n",
    "        s1 = set(label[\"accusation\"])\n",
    "        s2 = set()\n",
    "        for name in truth[\"accusation\"]:\n",
    "            s2.add(self.accu_dic[name.replace(\"[\", \"\").replace(\"]\", \"\")])\n",
    "\n",
    "        for a in range(0, self.task1_cnt):\n",
    "            in1 = (a + 1) in s1\n",
    "            in2 = (a + 1) in s2\n",
    "            if in1:\n",
    "                if in2:\n",
    "                    result[0][a][\"TP\"] += 1\n",
    "                else:\n",
    "                    result[0][a][\"FP\"] += 1\n",
    "            else:\n",
    "                if in2:\n",
    "                    result[0][a][\"FN\"] += 1\n",
    "                else:\n",
    "                    result[0][a][\"TN\"] += 1\n",
    "\n",
    "        s1 = set(label[\"articles\"])\n",
    "        s2 = set()\n",
    "        for name in truth[\"relevant_articles\"]:\n",
    "            s2.add(self.law_dic[name])\n",
    "\n",
    "        for a in range(0, self.task2_cnt):\n",
    "            in1 = (a + 1) in s1\n",
    "            in2 = (a + 1) in s2\n",
    "            if in1:\n",
    "                if in2:\n",
    "                    result[1][a][\"TP\"] += 1\n",
    "                else:\n",
    "                    result[1][a][\"FP\"] += 1\n",
    "            else:\n",
    "                if in2:\n",
    "                    result[1][a][\"FN\"] += 1\n",
    "                else:\n",
    "                    result[1][a][\"TN\"] += 1\n",
    "\n",
    "        result[2][\"cnt\"] += 1\n",
    "        sc = 0\n",
    "        if truth[\"term_of_imprisonment\"][\"death_penalty\"]:\n",
    "            if label[\"imprisonment\"] == -2:\n",
    "                sc = 1\n",
    "        elif truth[\"term_of_imprisonment\"][\"life_imprisonment\"]:\n",
    "            if label[\"imprisonment\"] == -1:\n",
    "                sc = 1\n",
    "        else:\n",
    "            if label[\"imprisonment\"] < 0:\n",
    "                sc = 0\n",
    "            else:\n",
    "                v1 = truth[\"term_of_imprisonment\"][\"imprisonment\"]\n",
    "                v2 = label[\"imprisonment\"]\n",
    "                v = abs(log(v1 + 1) - log(v2 + 1))\n",
    "                if v <= 0.2:\n",
    "                    sc = 1\n",
    "                elif v <= 0.4:\n",
    "                    sc = 0.8\n",
    "                elif v <= 0.6:\n",
    "                    sc = 0.6\n",
    "                elif v <= 0.8:\n",
    "                    sc = 0.4\n",
    "                elif v <= 1.0:\n",
    "                    sc = 0.2\n",
    "                else:\n",
    "                    sc = 0\n",
    "        sc = sc * 1.0\n",
    "        result[2][\"score\"] += sc\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Calculate precision, recall and f1 value\n",
    "    # According to https://github.com/dice-group/gerbil/wiki/Precision,-Recall-and-F1-measure\n",
    "    @staticmethod\n",
    "    def get_value(res):\n",
    "        if res[\"TP\"] == 0:\n",
    "            if res[\"FP\"] == 0 and res[\"FN\"] == 0:\n",
    "                precision = 1.0\n",
    "                recall = 1.0\n",
    "                f1 = 1.0\n",
    "            else:\n",
    "                precision = 0.0\n",
    "                recall = 0.0\n",
    "                f1 = 0.0\n",
    "        else:\n",
    "            precision = 1.0 * res[\"TP\"] / (res[\"TP\"] + res[\"FP\"])\n",
    "            recall = 1.0 * res[\"TP\"] / (res[\"TP\"] + res[\"FN\"])\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "        return precision, recall, f1\n",
    "\n",
    "    # Generate score for the first two subtasks\n",
    "    def gen_score(self, arr):\n",
    "        sumf = 0\n",
    "        y = {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"TN\": 0}\n",
    "        for x in arr:\n",
    "            p, r, f = self.get_value(x)\n",
    "            sumf += f\n",
    "            for z in x.keys():\n",
    "                y[z] += x[z]\n",
    "\n",
    "        _, __, f_ = self.get_value(y)\n",
    "\n",
    "        return (f_ + sumf * 1.0 / len(arr)) / 2.0\n",
    "\n",
    "    # Generatue all scores\n",
    "    def get_score(self, result):\n",
    "        s1 = self.gen_score(result[0])\n",
    "        s2 = self.gen_score(result[1])\n",
    "        s3 = 1.0 * result[2][\"score\"] / result[2][\"cnt\"]\n",
    "        return [s1, s2, s3]\n",
    "\n",
    "    # Test with ground truth path and the user's output path\n",
    "    def test(self, truth_path, output_path):\n",
    "        cnt = 0\n",
    "        result = [[], [], {}]\n",
    "        for a in range(0, self.task1_cnt):\n",
    "            result[0].append({\"TP\": 0, \"FP\": 0, \"TN\": 0, \"FN\": 0})\n",
    "        for a in range(0, self.task2_cnt):\n",
    "            result[1].append({\"TP\": 0, \"FP\": 0, \"TN\": 0, \"FN\": 0})\n",
    "        result[2] = {\"cnt\": 0, \"score\": 0}\n",
    "\n",
    "        for file_name in os.listdir(truth_path):\n",
    "            inf = open(os.path.join(truth_path, file_name), \"r\")\n",
    "            ouf = open(os.path.join(output_path, file_name), \"r\")\n",
    "\n",
    "            for line in inf:\n",
    "                ground_truth = json.loads(line)[\"meta\"]\n",
    "                user_output = json.loads(ouf.readline())\n",
    "\n",
    "                cnt += 1\n",
    "                result = self.gen_new_result(result, ground_truth, user_output)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

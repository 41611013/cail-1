{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeshe/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf8 -*-\n",
    "'''\n",
    "Jessie\n",
    "'''\n",
    "\n",
    "import jieba\n",
    "import pickle\n",
    "# import cPickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from itertools import islice\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import os\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Input,Flatten,Dense, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.optimizers import Adagrad,RMSprop, Adam,SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "import h5sparse\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def init():\n",
    "    f = open('./CAIL_data/law.txt', 'r', encoding = 'utf8')\n",
    "    law = {}\n",
    "    lawname = {}\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        lawname[len(law)] = line.strip()\n",
    "        law[line.strip()] = len(law)\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    f = open('./CAIL_data/accu.txt', 'r', encoding = 'utf8')\n",
    "    accu = {}\n",
    "    accuname = {}\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        accuname[len(accu)] = line.strip()\n",
    "        accu[line.strip()] = len(accu)\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    return law, accu, lawname, accuname\n",
    "\n",
    "\n",
    "law, accu, lawname, accuname = init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClassNum(kind):\n",
    "    global law\n",
    "    global accu\n",
    "\n",
    "    if kind == 'law':\n",
    "        return len(law)\n",
    "    if kind == 'accu':\n",
    "        return len(accu)\n",
    "\n",
    "def getName(index, kind):\n",
    "    global lawname\n",
    "    global accuname\n",
    "    if kind == 'law':\n",
    "        return lawname[index]\n",
    "\n",
    "    if kind == 'accu':\n",
    "        return accuname[index]\n",
    "\n",
    "\n",
    "def gettime(time):\n",
    "    #将刑期用分类模型来做\n",
    "    v = int(time['imprisonment'])\n",
    "\n",
    "    if time['death_penalty']:\n",
    "        return 0\n",
    "    if time['life_imprisonment']:\n",
    "        return 1\n",
    "    elif v > 10 * 12:\n",
    "        return 2\n",
    "    elif v > 7 * 12:\n",
    "        return 3\n",
    "    elif v > 5 * 12:\n",
    "        return 4\n",
    "    elif v > 3 * 12:\n",
    "        return 5\n",
    "    elif v > 2 * 12:\n",
    "        return 6\n",
    "    elif v > 1 * 12:\n",
    "        return 7\n",
    "    else:\n",
    "        return 8\n",
    "\n",
    "\n",
    "def getlabel(d, kind):\n",
    "    global law\n",
    "    global accu    \n",
    "    # 做单标签\n",
    "    if kind == 'law':\n",
    "    # 返回多个类的第一个\n",
    "        return law[str(d['meta']['relevant_articles'][0])]\n",
    "    if kind == 'accu':\n",
    "        return accu[d['meta']['accusation'][0]]    \n",
    "    if kind == 'time':\n",
    "        return gettime(d['meta']['term_of_imprisonment'])\n",
    "    \n",
    "    return label\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "\n",
    "dim = 300\n",
    "\n",
    "def cut_text(alltext):\n",
    "    train_text = []\n",
    "    for text in alltext:\n",
    "        train_text.append(' '.join(jieba.cut(text)))\n",
    "     \n",
    "    return train_text\n",
    "\n",
    "def read_trainData(path):\n",
    "    fin = open(path, 'r', encoding = 'utf8')\n",
    "    \n",
    "    alltext = []\n",
    "    \n",
    "    accu_label = []\n",
    "    law_label = []\n",
    "    time_label = []\n",
    "\n",
    "    line = fin.readline()\n",
    "    while line:\n",
    "        d = json.loads(line)\n",
    "        alltext.append(d['fact'])\n",
    "        accu_label.append(getlabel(d, 'accu'))\n",
    "        law_label.append(getlabel(d, 'law'))\n",
    "        time_label.append(getlabel(d, 'time'))\n",
    "        line = fin.readline()\n",
    "    fin.close()\n",
    "\n",
    "    return alltext, accu_label, law_label, time_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################测试部分###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1], [], []]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "texts = ['a a a', 'b b', 'c']\n",
    "tokenizer = Tokenizer(num_words=2)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "tokenizer.word_index\n",
    "tokenizer.texts_to_sequences(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts2 = ['m m m', 'b']\n",
    "tokenizer.fit_on_texts(texts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = ['n n', 'b', 'a a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [], [1, 1]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# loading\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer2 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2.texts_to_sequences(texts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 2, 'c': 4, 'm': 3}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.texts_to_sequences(['i a m m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Found %s unique tokens.\" % len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################测试部分结束——不用运行######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcess(object):\n",
    "\n",
    "    def __init__(self, path, data_path, max_sequence_length = 250, embedding_dim=1000):\n",
    "        self.path = path                # 数据路径\n",
    "        self.data_path = data_path\n",
    "        self.max_sequence_length = max_sequence_length   # 最长文本长度, 每个fact有多少个word_index [1, 23,,...],多出的补零\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_index = {}           # 单词与嵌入向量映射 [word +vec] \n",
    "        self.embedding_matrix = None    # 嵌入矩阵 [word_index]\n",
    "        self.word_index = {}   # 单词索引\n",
    "        \n",
    "        self.alltext = None     # 训练facts        \n",
    "        self.accu_label = None    #罪行标签\n",
    "        self.law_label = None     #发条标签\n",
    "        self.time_label = None    #判刑标签        \n",
    "        self.texts = [] #分词后的训练样本\n",
    "\n",
    "        self.converTextToTensor(self.path + 'predictor/model/tokenizer.pickle', self.path +'word_index.pkl')\n",
    "        self.processEmbedding(self.path+'word_index.pkl', self.path +'embedding_matrix.pkl', self.path+'w2v.pkl')\n",
    "\n",
    "    #format our text samples and labels into tensors that can be fed into a neural network.\n",
    "    def converTextToTensor(self, tokenizer_path = './model/tokenizer.pickle',\n",
    "                           word_index_path= './word_index.pkl'):\n",
    "        print('call member convertTextToTensor')\n",
    "        self.alltext, self.accu_label, self.law_label, self.time_label = read_trainData(data_path)\n",
    "        self.texts = cut_text(self.alltext)  # [['','',''], ['','','']]\n",
    "            \n",
    "        tokenizer = Tokenizer(num_words=10000) #出现次数top10000才会用来index\n",
    "        tokenizer.fit_on_texts(self.texts)\n",
    "        with open(tokenizer_path, 'wb') as handle:\n",
    "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        self.word_index = tokenizer.word_index  #{word:index,...}，索引index从1开始\n",
    "        print(\"Found %s unique tokens.\" % len(self.word_index))\n",
    "        with open(word_index_path, 'wb') as f:\n",
    "            pickle.dump(self.word_index,f, -1)\n",
    "        f.close()\n",
    "        \n",
    "        \n",
    "        sequence = tokenizer.texts_to_sequences(self.texts)  # 将文本转换成整数序列            \n",
    "        self.texts = pad_sequences(sequence, maxlen=self.max_sequence_length, padding= 'post', truncating = 'post')\n",
    "        #[[1, 1, 3], [2, 1, 0]] #此为输入\n",
    "        print('pad done')\n",
    "\n",
    "    #compute the embedding and mapping the words/word_index to known embeddings\n",
    "    def processEmbedding(self, word_index_path = './word_index.pkl', \n",
    "                         embedding_path = './embedding_matrix.pkl', word2vec_path = './w2v.model'):\n",
    "        print('call processEmbedding')\n",
    "        if os.path.isfile(embedding_path):\n",
    "            print('load embedding matrix')\n",
    "            with open(embedding_path, 'rb') as f:\n",
    "                self.embedding_matrix = pickle.load(f)\n",
    "        else:\n",
    "            print('calculate embedding matrix')\n",
    "            facts = self.alltext\n",
    "            sentences = [[word for word in fact.split()] for fact in facts]\n",
    "            word_model = gensim.models.Word2Vec(sentences, size= self.embedding_dim, min_count = 10, window = 5)\n",
    "            word_model.save(word2vec_path)\n",
    "            joblib.dump(word_model, './predictor/model/word_model2.model')\n",
    "#             print('saving word embeddingmodel')\n",
    "            \n",
    "#             word_model = gensim.models.Word2Vec.load(word2vec_path) \n",
    "#             word_model2 = joblib.load('./predictor/model/word_model2.model')\n",
    "#             print('word_model built completed.')\n",
    "\n",
    "            with open(word_index_path, 'rb') as f:\n",
    "                dic = pickle.load(f)\n",
    "                self.embedding_matrix = np.zeros((len(self.word_index) + 1, self.embedding_dim))\n",
    "                for word, i in dic.items():\n",
    "                    try:\n",
    "                        embedding_vector = word_model[word]\n",
    "                    except:\n",
    "                        embedding_vector = None\n",
    "                    if embedding_vector is not None:\n",
    "                        self.embedding_matrix[i] = embedding_vector \n",
    "                        # self.embedding_index[word] = word_model[word]                         \n",
    "            print('Result embedding shape:', self.embedding_matrix.shape)\n",
    "            \n",
    "            with open(embedding_path, 'wb') as f:\n",
    "                pickle.dump(self.embedding_matrix, f, -1)          \n",
    "            print(\"Found %s word vectors.\" % len(self.embedding_matrix))\n",
    "#             with open(path_to_save, 'wb') as f:\n",
    "#                 pickle.dump(self.embeddings_index, f)\n",
    "#         self.max_nb_words = len(self.embedding_matrix)\n",
    "\n",
    "#     def word2idx(word):\n",
    "#         return word_model.wv.vocab[word].index\n",
    "#     def idx2word(idx):\n",
    "#         return word_model.wv.index2word[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(embedding_path):\n",
    "    print('load embedding matrix')\n",
    "    with open(embedding_path, 'rb') as f:\n",
    "        self.embedding_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCNN(max_sequence_length, word_index, embedding_dim, embedding_matrix, filter_sizes = [3,4,5], outpu_dim=1):\n",
    "\n",
    "    print('call getCNN')\n",
    "    convs = []\n",
    "    # 输入层次\n",
    "    sequence_input = Input(shape = (max_sequence_length,), dtype= 'int32', name = 'input')\n",
    "\n",
    "    # 嵌入层\n",
    "    embedding_layer = Embedding(len(word_index)+1,\n",
    "                                embedding_dim,\n",
    "                                weights= [embedding_matrix],\n",
    "                                input_length= max_sequence_length,\n",
    "                                trainable= False)\n",
    "    embedded_sequence = embedding_layer(sequence_input)\n",
    "\n",
    "    for fsz in filter_sizes:\n",
    "        conv = Conv1D(32, kernel_size= fsz, activation='relu')(embedded_sequence)\n",
    "        pool = MaxPooling1D(2)(conv)\n",
    "        convs.append(pool)\n",
    "    merge1 = concatenate(convs, axis=1)\n",
    "    conv1 = Conv1D(32,4, activation='relu')(merge1)\n",
    "    pool1 = MaxPooling1D(5)(conv1)\n",
    "    flat = Flatten()(pool1)\n",
    "    dense = Dense(32,activation='relu')(flat)\n",
    "    preds = Dense(output_dim, activation='sigmoid', name='pred')(dense)\n",
    "\n",
    "    model = Model(inputs=sequence_input,outputs=preds)\n",
    "\n",
    "    return model\n",
    "\n",
    "def generate_batch_data(input, output, batch_size):\n",
    "\n",
    "    i = 0\n",
    "    while 1:\n",
    "        yield (input[i:i+batch_size, :],np.array(output[i: i+batch_size,:].todense()))\n",
    "        i += batch_size\n",
    "        if (i+ batch_size) > input.shape[0]:\n",
    "            i = 0\n",
    "\n",
    "\n",
    "def validaton_generator(vali_input, vali_output, batch_size):\n",
    "    i = 0\n",
    "    while 1:\n",
    "        yield (vali_input[i:i + batch_size, :], np.array(vali_output[i: i + batch_size, :].todense()))\n",
    "        i += batch_size\n",
    "        if (i + batch_size) > vali_input.shape[0]:\n",
    "            i = 0\n",
    "\n",
    "def train_CNN(vecs, label, batch_size = 2000):            \n",
    "    #  模型学习\n",
    "    # 构建一个CNN模型\n",
    "    model = getCNN(dataSet.max_sequence_length,\n",
    "                   dataSet.word_index,\n",
    "                   dataSet.embedding_dim,\n",
    "                   dataSet.embedding_matrix)\n",
    "\n",
    "    print('compile')\n",
    "    # 编译CNN模型\n",
    "    if learner.lower() == \"adagrad\":\n",
    "        model.compile(optimizer=Adagrad(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    elif learner.lower() == \"rmsprop\":\n",
    "        model.compile(optimizer=RMSprop(lr=learning_rate), loss='binary_crossentropy',metrics= ['accuracy'])\n",
    "    elif learner.lower() == \"adam\":\n",
    "        model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    else:\n",
    "        model.compile(optimizer=SGD(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "    accu_model = model.fit_generator(generate_batch_data(np.array(vecs)[0:10000,:],label[0:10000,:], batch_size),\n",
    "                               steps_per_epoch=np.array(vecs)[0:10000,:].shape[0] / batch_size , epochs=1,\n",
    "                               callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call member convertTextToTensor\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    max_sequence_length = 250\n",
    "    embedding_dim = 2000\n",
    "    output_dim = 1 # 先做单模型的\n",
    "\n",
    "    #CNN模型超参\n",
    "    learner = 'rmsprop'\n",
    "    learning_rate = 1e-3\n",
    "    batch_size = 2000\n",
    "\n",
    "    # 构造数据集\n",
    "    path = './'\n",
    "    data_path = './CAIL_data/data_train.json'\n",
    "    dataSet = DataProcess(path, data_path, max_sequence_length, embedding_dim)\n",
    " \n",
    "    \n",
    "\n",
    "    print('starting fitting')\n",
    "\n",
    "    print('accu CNN')\n",
    "    accu = train_CNN(dataSet.texts, dataSet.accu_label, batch_size)\n",
    "    print('law CNN')\n",
    "    law = train_CNN(dataSet.texts, dataSet.law_label, batch_size)\n",
    "    print('time CNN')\n",
    "    time = train_SVC(dataSet.texts, dataSet.time_label, batch_size)\n",
    "\n",
    "\n",
    "    joblib.dump(accu, 'predictor/model/accu.model')\n",
    "    joblib.dump(law, 'predictor/model/law.model')\n",
    "    joblib.dump(time, 'predictor/model/time.model')\n",
    "    \n",
    "    # 模型指标\n",
    "    print('fit done')\n",
    "    for key, value in hist.history.items():\n",
    "        print(key, value)\n",
    "    # 保存模型\n",
    "    model.save('model1.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################预测部分#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class testDataProcess(object):\n",
    "\n",
    "#     def __init__(self, content, max_sequence_length = 250, embedding_dim = 1000, \n",
    "#                  word_index_path = './word_index.pkl', \n",
    "#                  embedding_matrix = './embedding_matrix.pkl'):\n",
    "        \n",
    "#         self.max_sequence_length = max_sequence_length   # 最长文本长度, 每个fact有多少个word_index [1, 23,,...],多出的补零\n",
    "#         self.embedding_matrix = None    # 嵌入矩阵 [word_index]\n",
    "#         self.word_index = {}   # 单词索引\n",
    "#         self.embedding_dim = None\n",
    "#         self.alltext = content    # 训练facts            \n",
    "#         self.texts = [] #分词后的训练样本\n",
    "\n",
    "#         self.converTextToTensor(self.path +'word_index.pkl')\n",
    "\n",
    "#     def converTextToTensor(self, word_index_path= './tokenizer.pickle.pkl'):\n",
    "#         print('load tokenizer')\n",
    "#         with open('tokenizer.pickle', 'rb') as handle:\n",
    "#             tokenizer = pickle.load(handle)\n",
    "            \n",
    "#         self.texts = cut_text(self.alltext)  # [['','',''], ['','','']]\n",
    "            \n",
    "#         sequence = tokenizer.texts_to_sequences(self.texts)  # 将文本转换成整数序列            \n",
    "#         self.texts = pad_sequences(sequence, maxlen=self.max_sequence_length, padding= 'post', truncating = 'post')\n",
    "#         #[[1, 1, 3], [2, 1, 0]] #此为输入\n",
    "#         print('pad done')\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jieba\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "\n",
    "class Predictor(object):\n",
    "    def __init__(self):\n",
    "        with open('predicotr/model/tokenizer.pickle', 'rb') as handle:\n",
    "            self.tokenizer = pickle.load(handle)\n",
    "        self.law = joblib.load('predictor/model/law.model')\n",
    "        self.accu = joblib.load('predictor/model/accu.model')\n",
    "        self.time = joblib.load('predictor/model/time.model')\n",
    "        self.batch_size = 1\n",
    "\n",
    "    def predict_law(self, vec):\n",
    "        y = self.law.predict(vec)\n",
    "        return [y[0] + 1]\n",
    "    \n",
    "    def predict_accu(self, vec):\n",
    "        y = self.accu.predict(vec)\n",
    "        return [y[0] + 1]\n",
    "    \n",
    "    def predict_time(self, vec):\n",
    "\n",
    "        y = self.time.predict(vec)[0]\n",
    "        \n",
    "        #返回每一个罪名区间的中位数\n",
    "        if y == 0:\n",
    "            return -2\n",
    "        if y == 1:\n",
    "            return -1\n",
    "        if y == 2:\n",
    "            return 120\n",
    "        if y == 3:\n",
    "            return 102\n",
    "        if y == 4:\n",
    "            return 72\n",
    "        if y == 5:\n",
    "            return 48\n",
    "        if y == 6:\n",
    "            return 30\n",
    "        if y == 7:\n",
    "            return 18\n",
    "        else:\n",
    "            return 6\n",
    "    \n",
    "    \n",
    "    def predict(self, content):\n",
    "\n",
    "            \n",
    "        self.texts = cut_text(self.alltext)  # [['','',''], ['','','']]\n",
    "            \n",
    "        sequence = self.tokenizer.texts_to_sequences(self.texts)  # 将文本转换成整数序列            \n",
    "        self.texts = pad_sequences(sequence, maxlen=self.max_sequence_length, padding= 'post', truncating = 'post')\n",
    "        #[[1, 1, 3], [2, 1, 0]] #此为输入\n",
    "        print('pad done')\n",
    "\n",
    "        vec = self.texts\n",
    "        ans = {}\n",
    "\n",
    "        ans['accusation'] = self.predict_accu(vec)\n",
    "        ans['articles'] = self.predict_law(vec)\n",
    "        ans['imprisonment'] = self.predict_time(vec)\n",
    "        \n",
    "        #print(ans)\n",
    "        return [ans]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "#from predictor import Predictor\n",
    "\n",
    "data_path = \"./input/\"  # The directory of the input data\n",
    "output_path = \"./output/\"  # The directory of the output data\n",
    "\n",
    "\n",
    "def format_result(result):\n",
    "    rex = {\"accusation\": [], \"articles\": [], \"imprisonment\": -3}\n",
    "\n",
    "    res_acc = []\n",
    "    for x in result[\"accusation\"]:\n",
    "        if not (x is None):\n",
    "            res_acc.append(int(x))\n",
    "    rex[\"accusation\"] = res_acc\n",
    "\n",
    "    if not (result[\"imprisonment\"] is None):\n",
    "        rex[\"imprisonment\"] = int(result[\"imprisonment\"])\n",
    "    else:\n",
    "        rex[\"imprisonment\"] = -3\n",
    "\n",
    "    res_art = []\n",
    "    for x in result[\"articles\"]:\n",
    "        if not (x is None):\n",
    "            res_art.append(int(x))\n",
    "    rex[\"articles\"] = res_art\n",
    "\n",
    "    return rex\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user = Predictor()\n",
    "    cnt = 0\n",
    "\n",
    "\n",
    "    def get_batch():\n",
    "        v = user.batch_size\n",
    "        if not (type(v) is int) or v <= 0:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return v\n",
    "\n",
    "\n",
    "    def solve(fact):\n",
    "        result = user.predict(fact)\n",
    "\n",
    "        for a in range(0, len(result)):\n",
    "            result[a] = format_result(result[a])\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    for file_name in os.listdir(data_path):\n",
    "        inf = open(os.path.join(data_path, file_name), \"r\")\n",
    "        ouf = open(os.path.join(output_path, file_name), \"w\")\n",
    "\n",
    "        fact = []\n",
    "\n",
    "        for line in inf:\n",
    "            fact.append(json.loads(line)[\"fact\"])\n",
    "            if len(fact) == get_batch():\n",
    "                result = solve(fact)\n",
    "                cnt += len(result)\n",
    "                for x in result:\n",
    "                    print(json.dumps(x), file=ouf)\n",
    "                fact = []\n",
    "\n",
    "        if len(fact) != 0:\n",
    "            result = solve(fact)\n",
    "            cnt += len(result)\n",
    "            for x in result:\n",
    "                print(json.dumps(x), file=ouf)\n",
    "            fact = []\n",
    "\n",
    "        ouf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
